{"cells":[{"cell_type":"markdown","metadata":{"id":"qIPw1gOOFzYg"},"source":["<h1 align=\"center\">Lab 2:  Sexism Identification in Twitter</h1>\n","<h2 align=\"center\">Session 4. Transformers and Explainability</h2>\n","<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Natural Language and Information Retrieval</h3>\n","<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Degree in Data Science</h3>\n","<h3 style=\"display:block; margin-top:5px;\" align=\"center\">2024-2025</h3>    \n","<h3 style=\"display:block; margin-top:5px;\" align=\"center\">ETSInf. Universitat Politècnica de València</h3>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"lbN033q9FzYj"},"source":["### Put your names here\n","\n","- Jaime Ballester Solá\n","- Marc Romeu Ferras"]},{"cell_type":"markdown","metadata":{"id":"jGkg5dmiFzYk"},"source":["### CONSTANTS"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"11w0AotjFzYl","executionInfo":{"status":"ok","timestamp":1747767124218,"user_tz":-120,"elapsed":12,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["COLAB = True # IF YOU USE GOOGLE COLAB -> COLAB = True\n","PIP = True # IF YOU NEED INSTALL LIBRARIES -> PIP = True"]},{"cell_type":"markdown","metadata":{"id":"MQgdzjiiFzYm"},"source":["**If you have trouble installing ferret-xai or jsonnet on Linux, try installing these packages first:**\n","\n","- sudo apt install cmake   \n","- sudo apt install g++     \n","- sudo apt install make\n","\n","**Or that after installing ferret-xai:**\n","\n","- conda install -c conda-forge libstdcxx-ng --update-deps\n"]},{"cell_type":"markdown","metadata":{"id":"ogBGl78iFzYm"},"source":["## Some libraries"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"mwCKDi3rFzYn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1747767276111,"user_tz":-120,"elapsed":37037,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}},"outputId":"31ad9336-5afb-49b6-c7e7-b7c0b2e3b0f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n","Collecting pip\n","  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n","Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 24.1.2\n","    Uninstalling pip-24.1.2:\n","      Successfully uninstalled pip-24.1.2\n","Successfully installed pip-25.1.1\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.21.0)\n","Collecting datasets\n","  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n","Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n","Installing collected packages: datasets\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.21.0\n","    Uninstalling datasets-2.21.0:\n","      Successfully uninstalled datasets-2.21.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ferret-xai 0.4.2 requires datasets<3.0.0,>=2.16.1, but you have datasets 3.6.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed datasets-3.6.0\n","Requirement already satisfied: ferret-xai in /usr/local/lib/python3.11/dist-packages (0.4.2)\n","Requirement already satisfied: captum<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from ferret-xai) (0.7.0)\n","Collecting datasets<3.0.0,>=2.16.1 (from ferret-xai)\n","  Using cached datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: joblib<2.0.0,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from ferret-xai) (1.5.0)\n","Requirement already satisfied: lime<0.3.0.0,>=0.2.0.1 in /usr/local/lib/python3.11/dist-packages (from ferret-xai) (0.2.0.1)\n","Requirement already satisfied: matplotlib<4.0.0,>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from ferret-xai) (3.10.0)\n","Requirement already satisfied: numpy<2.0.0,>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from ferret-xai) (1.26.4)\n","Requirement already satisfied: opencv-python<5.0.0.0,>=4.9.0.80 in /usr/local/lib/python3.11/dist-packages (from ferret-xai) (4.11.0.86)\n","Requirement already satisfied: pandas<3.0.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from ferret-xai) (2.2.2)\n","Requirement already satisfied: pytreebank<0.3.0,>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ferret-xai) (0.2.7)\n","Requirement already satisfied: scikit-image<0.22.0,>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from ferret-xai) (0.21.0)\n","Requirement already satisfied: seaborn<0.14.0,>=0.13.1 in /usr/local/lib/python3.11/dist-packages (from ferret-xai) (0.13.2)\n","Requirement already satisfied: sentencepiece<0.2.0,>=0.1.99 in /usr/local/lib/python3.11/dist-packages (from ferret-xai) (0.1.99)\n","Requirement already satisfied: shap<0.45.0,>=0.44.0 in /usr/local/lib/python3.11/dist-packages (from ferret-xai) (0.44.1)\n","Requirement already satisfied: thermostat-datasets<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from ferret-xai) (1.1.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from ferret-xai) (4.67.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.36.2 in /usr/local/lib/python3.11/dist-packages (from ferret-xai) (4.52.0)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.11/dist-packages (from captum<0.8.0,>=0.7.0->ferret-xai) (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.16.1->ferret-xai) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.16.1->ferret-xai) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.16.1->ferret-xai) (0.3.7)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.16.1->ferret-xai) (2.32.3)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.16.1->ferret-xai) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.16.1->ferret-xai) (0.70.15)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<3.0.0,>=2.16.1->ferret-xai) (2024.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.16.1->ferret-xai) (3.11.15)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.16.1->ferret-xai) (0.31.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.16.1->ferret-xai) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets<3.0.0,>=2.16.1->ferret-xai) (6.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lime<0.3.0.0,>=0.2.0.1->ferret-xai) (1.15.3)\n","Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from lime<0.3.0.0,>=0.2.0.1->ferret-xai) (1.6.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.4->ferret-xai) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.4->ferret-xai) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.4->ferret-xai) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.4->ferret-xai) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.4->ferret-xai) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.4->ferret-xai) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.7.4->ferret-xai) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.0.3->ferret-xai) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.0.3->ferret-xai) (2025.2)\n","Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.22.0,>=0.21.0->ferret-xai) (3.4.2)\n","Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.22.0,>=0.21.0->ferret-xai) (2.37.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.22.0,>=0.21.0->ferret-xai) (2025.5.10)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.22.0,>=0.21.0->ferret-xai) (1.8.0)\n","Requirement already satisfied: lazy_loader>=0.2 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.22.0,>=0.21.0->ferret-xai) (0.4)\n","Requirement already satisfied: slicer==0.0.7 in /usr/local/lib/python3.11/dist-packages (from shap<0.45.0,>=0.44.0->ferret-xai) (0.0.7)\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from shap<0.45.0,>=0.44.0->ferret-xai) (0.60.0)\n","Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap<0.45.0,>=0.44.0->ferret-xai) (3.1.1)\n","Requirement already satisfied: jsonnet in /usr/local/lib/python3.11/dist-packages (from thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (0.21.0)\n","Requirement already satisfied: overrides in /usr/local/lib/python3.11/dist-packages (from thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (7.7.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (5.29.4)\n","Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.11/dist-packages (from thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (0.5.2)\n","Requirement already satisfied: spacy>=3.0 in /usr/local/lib/python3.11/dist-packages (from thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (3.8.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.36.2->ferret-xai) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.36.2->ferret-xai) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.36.2->ferret-xai) (0.5.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.2->datasets<3.0.0,>=2.16.1->ferret-xai) (4.13.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.16.1->ferret-xai) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.16.1->ferret-xai) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.16.1->ferret-xai) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.16.1->ferret-xai) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.16.1->ferret-xai) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.16.1->ferret-xai) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<3.0.0,>=2.16.1->ferret-xai) (1.20.0)\n","Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets<3.0.0,>=2.16.1->ferret-xai) (3.10)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.7.4->ferret-xai) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<3.0.0,>=2.16.1->ferret-xai) (3.4.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<3.0.0,>=2.16.1->ferret-xai) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<3.0.0,>=2.16.1->ferret-xai) (2025.4.26)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime<0.3.0.0,>=0.2.0.1->ferret-xai) (3.6.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (1.0.12)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (3.0.9)\n","Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (8.3.4)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (0.15.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (2.11.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (3.1.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (75.2.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (3.5.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (0.4.0)\n","Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (1.2.1)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (8.2.0)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (0.21.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (7.1.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (1.17.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (2.19.1)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (0.1.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6->captum<0.8.0,>=0.7.0->ferret-xai) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy>=3.0->thermostat-datasets<2.0.0,>=1.1.0->ferret-xai) (3.0.2)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->shap<0.45.0,>=0.44.0->ferret-xai) (0.43.0)\n","Using cached datasets-2.21.0-py3-none-any.whl (527 kB)\n","Installing collected packages: datasets\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 3.6.0\n","    Uninstalling datasets-3.6.0:\n","      Successfully uninstalled datasets-3.6.0\n","Successfully installed datasets-2.21.0\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"]}],"source":["if PIP:\n","    !pip install pip --upgrade\n","    !pip install transformers --upgrade\n","    !pip  install datasets accelerate --upgrade\n","    !pip install ferret-xai --upgrade\n","    !pip install tqdm\n","    !pip install transformers"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"bG1vkzS1FzYn","executionInfo":{"status":"error","timestamp":1747767276328,"user_tz":-120,"elapsed":194,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}},"colab":{"base_uri":"https://localhost:8080/","height":876},"outputId":"df5eb57d-85de-491c-b397-aee81688dbb0"},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"Could not import module 'AutoTokenizer'. Are this object's requirements defined correctly?","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2072\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2073\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbeam_search\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeamScorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamSearchScorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConstrainedBeamSearchScorer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m from .candidate_generator import (\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0mAssistantVocabTranslatorCache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/candidate_generator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_sklearn_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_csr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from ._sputils import (asmatrix, check_reshape_kwargs, check_shape,\n\u001b[0m\u001b[1;32m      6\u001b[0m                        \u001b[0mget_sum_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misdense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/sparse/_sputils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_long\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_ulong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_namespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_numpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_docscrape\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_api_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m from scipy._lib.array_api_compat import (\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mis_array_api_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/_lib/array_api_compat/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# from numpy import * doesn't overwrite these builtin names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy.rec'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2044\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2074\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2075\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2076\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.generation.utils because of the following error (look up to see its traceback):\nNo module named 'numpy.rec'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2072\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2073\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_decoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoderDecoderConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mauto_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_LazyAutoMapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m from .configuration_auto import (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2047\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m                 raise ModuleNotFoundError(\n\u001b[0m\u001b[1;32m   2049\u001b[0m                     \u001b[0;34mf\"Could not import module '{name}'. Are this object's requirements defined correctly?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: Could not import module 'GenerationMixin'. Are this object's requirements defined correctly?","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2044\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2074\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2075\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2076\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.auto.tokenization_auto because of the following error (look up to see its traceback):\nCould not import module 'GenerationMixin'. Are this object's requirements defined correctly?","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-4760cae4dca0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mprecision_recall_fscore_support\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2046\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2048\u001b[0;31m                 raise ModuleNotFoundError(\n\u001b[0m\u001b[1;32m   2049\u001b[0m                     \u001b[0;34mf\"Could not import module '{name}'. Are this object's requirements defined correctly?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m                 ) from e\n","\u001b[0;31mModuleNotFoundError\u001b[0m: Could not import module 'AutoTokenizer'. Are this object's requirements defined correctly?","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import os\n","import sys\n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n","from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report, accuracy_score\n","from sklearn.preprocessing import LabelEncoder\n","import pandas as pd\n","from typing import List, Tuple\n","import tqdm"]},{"cell_type":"markdown","metadata":{"id":"mYC8asnzFzYo"},"source":["## Import readerEXIST2025 library, and read the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EcfiE8LRFzYo","executionInfo":{"status":"aborted","timestamp":1747767156351,"user_tz":-120,"elapsed":1,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["from pathlib import Path\n","\n","if COLAB is True:\n","  from google.colab import drive\n","  drive.mount('/content/drive',force_remount=True)\n","  base_path = \"/content/drive/MyDrive/EXISTS2025_TweetBusters\"\n","  library_path = base_path + \"/Functions\"\n","else:\n","  base_path = Path.cwd().parent\n","  library_path = base_path / \"Functions\"\n","\n","sys.path.insert(0, str(library_path))\n","from readerEXIST2025 import EXISTReader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NFfq4hrLFzYo","executionInfo":{"status":"aborted","timestamp":1747767156355,"user_tz":-120,"elapsed":32243,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["dataset_path = os.path.join(base_path, \"EXIST_2025_Dataset_V0.2/EXIST_2025_Dataset_V0.2/\")\n","\n","file_train = os.path.join(dataset_path, \"EXIST2025_training.json\")\n","file_dev = os.path.join(dataset_path, \"EXIST2025_dev.json\")\n","\n","reader_train = EXISTReader(file_train)\n","reader_dev = EXISTReader(file_dev)\n","\n","EnTrainTask1, EnDevTask1 = reader_train.get(lang=\"EN\", subtask=\"1\"), reader_dev.get(lang=\"EN\", subtask=\"1\")\n","SpTrainTask1, SpDevTask1 = reader_train.get(lang=\"ES\", subtask=\"1\"), reader_dev.get(lang=\"ES\", subtask=\"1\")"]},{"cell_type":"markdown","metadata":{"id":"EMP1sIooFzYp"},"source":["## Dataset class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qvi4uV_GFzYp","executionInfo":{"status":"aborted","timestamp":1747767156360,"user_tz":-120,"elapsed":32246,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["class SexismDataset(Dataset):\n","    def __init__(self, texts, labels, ids, tokenizer, max_len=128, pad=\"max_length\", trunc=True,rt='pt'):\n","        self.texts = texts.tolist()\n","        self.labels = labels\n","        self.ids = ids\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.pad = pad\n","        self.trunc = trunc\n","        self.rt = rt\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,padding=self.pad, truncation=self.trunc,\n","            return_tensors=self.rt\n","        )\n","\n","        return {\n","            'input_ids': inputs['input_ids'].flatten(),\n","            'attention_mask': inputs['attention_mask'].flatten(),\n","            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n","            'id': torch.tensor(self.ids[idx], dtype=torch.long)\n","        }"]},{"cell_type":"markdown","metadata":{"id":"bMd7u74hFzYp"},"source":["## Auxiliary functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gUCjv1vOny1c","executionInfo":{"status":"aborted","timestamp":1747767156362,"user_tz":-120,"elapsed":32246,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["from collections import Counter # Counter counts the number of occurrences of each item\n","from itertools import tee, count\n","\n","def uniquify(seq, suffs = count(1)):\n","    \"\"\"Make all the items unique by adding a suffix (1, 2, etc).\n","\n","    `seq` is mutable sequence of strings.\n","    `suffs` is an optional alternative suffix iterable.\n","    \"\"\"\n","    not_unique = [k for k,v in Counter(seq).items() if v > 1] # so we have: ['name', 'zip']\n","    # suffix generator dict - e.g., {'name': <my_gen>, 'zip': <my_gen>}\n","    suff_gens = dict(zip(not_unique, tee(suffs, len(not_unique))))\n","    for idx,s in enumerate(seq):\n","        try:\n","            suffix = str(next(suff_gens[s]))\n","        except KeyError:\n","            # s was unique\n","            continue\n","        else:\n","            seq[idx] += suffix\n","\n","def deduplicate(explanations):\n","    for i in range(len(explanations)):\n","        tokens = explanations[i].tokens\n","        uniquify(tokens, (f'_{x!s}' for x in range(1, 100)))\n","        explanations[i].tokens=tokens\n","    return explanations"]},{"cell_type":"markdown","metadata":{"id":"Lk_66ux0FzYq"},"source":["# Two options to predict"]},{"cell_type":"markdown","metadata":{"id":"u3kNaaINFzYq"},"source":["### The simplest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ww6HcO7VoVPd","executionInfo":{"status":"aborted","timestamp":1747767156363,"user_tz":-120,"elapsed":32245,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["import torch\n","import numpy as np\n","from transformers import Trainer, TrainingArguments\n","\n","\n","def predict_op1(model, dataset, args={}):\n","    \"\"\"\n","    Función predict_op1 modificada:\n","    - Permite especificar `output_dir` vía `args['output_dir']`\n","    - Añade parámetro `return_logits` para devolver los logits brutos si se desea\n","    \"\"\"\n","    # Configuración del Trainer\n","    training_args = TrainingArguments(\n","        output_dir=args.get(\"output_dir\", \"./output\"),\n","        per_device_eval_batch_size=args.get(\"per_device_eval_batch_size\", 16),\n","        do_train=False,\n","        do_eval=False,\n","    )\n","    trainer = Trainer(model=model, args=training_args)\n","\n","    # Realizar predicción\n","    predictions = trainer.predict(dataset)\n","    logits = predictions.predictions\n","\n","    # Cálculo de probabilidades con softmax\n","    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n","    # Clases predichas\n","    pred_classes = np.argmax(logits, axis=-1)\n","    # Probabilidades asociadas a la clase predicha\n","    pred_probs = probs[np.arange(len(probs)), pred_classes]\n","\n","    # Retornar logits si se solicita\n","    if args.get(\"return_logits\", False):\n","        return pred_classes, pred_probs, logits\n","    return pred_classes, pred_probs"]},{"cell_type":"markdown","metadata":{"id":"LnPOwhDKFzYq"},"source":["### The coolest, if you know what you're doing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q3qpb2FKFzYq","executionInfo":{"status":"aborted","timestamp":1747767156364,"user_tz":-120,"elapsed":32244,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["import torch\n","import numpy as np\n","from torch.utils.data import DataLoader\n","import tqdm\n","\n","def predict_op2(model, dataset, args={}):\n","    \"\"\"\n","    Versión mejorada de predict_op2:\n","    - Parámetro 'per_device_eval_batch_size' configurable vía args\n","    - Parámetro 'device' configurable vía args['device'], por defecto autodetección\n","    - Parámetro 'id_key' para eliminar clave de ID personalizada en el batch\n","    - Opción 'return_logits' para devolver también los logits crudos\n","    \"\"\"\n","    # Dispositivo\n","    device = args.get(\"device\", None)\n","    if device is None:\n","        if torch.cuda.is_available():\n","            device = torch.device(\"cuda\")\n","        elif torch.backends.mps.is_available():\n","            device = torch.device(\"mps\")\n","        else:\n","            device = torch.device(\"cpu\")\n","    model.to(device)\n","    model.eval()\n","\n","    # Tamaño de lote\n","    batch_size = args.get(\"per_device_eval_batch_size\", 16)\n","    dataloader = DataLoader(dataset, batch_size=batch_size)\n","\n","    logits_list = []\n","    with torch.no_grad():\n","        for batch in tqdm.notebook.tqdm(dataloader, desc=\"Predicting\"):\n","            # eliminar clave de ID si existe\n","            batch.pop(args.get(\"id_key\", \"id\"), None)\n","            # mover tensores\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            outputs = model(**batch)\n","            logits_list.append(outputs.logits.cpu())\n","\n","    # Concatenar logits\n","    logits = torch.cat(logits_list, dim=0)\n","    # Probabilidades\n","    probs = torch.nn.functional.softmax(logits, dim=-1).numpy()\n","    # Clases predichas\n","    pred_classes = np.argmax(probs, axis=-1)\n","    # Probabilidades de la clase predicha\n","    pred_probs = probs[np.arange(len(pred_classes)), pred_classes]\n","\n","    if args.get(\"return_logits\", False):\n","        return pred_classes, pred_probs, logits.numpy()\n","    return pred_classes, pred_probs"]},{"cell_type":"markdown","metadata":{"id":"m9Vv0vYEFzYq"},"source":["### Predictions from the best Spanish model"]},{"cell_type":"code","source":["from peft import LoraConfig, get_peft_model, TaskType\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    Trainer,\n","    TrainingArguments,\n","    EarlyStoppingCallback\n",")\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","import numpy as np\n","import pandas as pd\n","\n","def compute_metrics_1(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        labels, preds, average='binary', zero_division=0\n","    )\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall\n","    }\n","\n","def sexism_classification_pipeline_task1_LoRA(\n","    trainInfo, devInfo, testInfo=None,\n","    model_name='pysentimiento/robertuito-base-uncased',\n","    nlabels=2,\n","    ptype=\"single_label_classification\",\n","    **args\n","):\n","    # directorio donde se guarda el mejor modelo\n","    output_dir = \"/content/drive/MyDrive/PRACT4/modelo_final\"\n","\n","    # 1) tokenizer + modelo base\n","    labelEnc = LabelEncoder()\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        model_name,\n","        num_labels=nlabels,\n","        problem_type=ptype\n","    )\n","\n","    # 2) LoRA config\n","    lora_config = LoraConfig(\n","        task_type = args.get(\"task_type\", TaskType.SEQ_CLS),\n","        target_modules = args.get(\"target_modules\", [\"query\",\"value\"]),\n","        r = args.get(\"rank\", 32),\n","        lora_alpha = args.get(\"lora_alpha\", 16),\n","        lora_dropout = args.get(\"lora_dropout\", 0.1),\n","        bias = args.get(\"bias\", \"none\")\n","    )\n","    peft_model = get_peft_model(model, lora_config)\n","\n","    # 3) datasets\n","    train_dataset = SexismDataset(\n","        trainInfo[1],\n","        labelEnc.fit_transform(trainInfo[2]),\n","        [int(x) for x in trainInfo[0]],\n","        tokenizer\n","    )\n","    val_dataset = SexismDataset(\n","        devInfo[1],\n","        labelEnc.transform(devInfo[2]),\n","        [int(x) for x in devInfo[0]],\n","        tokenizer\n","    )\n","\n","    # 4) TrainingArguments\n","    training_args = TrainingArguments(\n","        report_to=\"none\",\n","        output_dir=output_dir,\n","        num_train_epochs=args.get('num_train_epochs', 3),\n","        learning_rate=args.get('learning_rate', 2e-5),\n","        per_device_train_batch_size=args.get('per_device_train_batch_size', 32),\n","        per_device_eval_batch_size=args.get('per_device_eval_batch_size', 32),\n","        fp16=True,\n","        gradient_checkpointing=True,\n","        logging_dir=args.get('logging_dir', './logs'),\n","        logging_steps=args.get('logging_steps', 100),\n","        eval_strategy='epoch',\n","        save_strategy='epoch',\n","        save_total_limit=1,\n","        load_best_model_at_end=True,\n","        metric_for_best_model='f1'\n","    )\n","\n","    trainer = Trainer(\n","        model=peft_model,\n","        args=training_args,\n","        train_dataset=train_dataset,\n","        eval_dataset=val_dataset,\n","        compute_metrics=compute_metrics_1,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=args.get(\"early_stopping_patience\", 2))]\n","    )\n","\n","    # 5) Entrena y evalúa\n","    trainer.train()\n","    eval_results = trainer.evaluate()\n","    print(\"Validation Results:\", eval_results)\n","\n","    # 6) Guarda el mejor checkpoint + modelo final\n","    trainer.save_model(output_dir)                        # guarda peft_model\n","    mixModel = peft_model.merge_and_unload()\n","    mixModel.save_pretrained(output_dir)                  # guarda modelo combinado\n","\n","    # 7) Si hay testInfo, predice y guarda csv\n","    if testInfo is not None:\n","        test_dataset = SexismDataset(\n","            testInfo[1],\n","            [0] * len(testInfo[1]),\n","            [int(x) for x in testInfo[0]],\n","            tokenizer\n","        )\n","        predictions = trainer.predict(test_dataset)\n","        preds = np.argmax(predictions.predictions, axis=1)\n","        df = pd.DataFrame({\n","            'id': testInfo[0],\n","            'label': labelEnc.inverse_transform(preds),\n","            \"test_case\": [\"EXIST2025\"] * len(preds)\n","        })\n","        df.to_csv('sexism_predictions_task1.csv', index=False)\n","        print(\"Prediction for TASK 1 completed. Results saved to sexism_predictions_task1.csv\")\n","        return mixModel, df\n","\n","    return mixModel, eval_results\n","\n","# Ejemplo de ejecución:\n","modelname = \"pysentimiento/robertuito-base-uncased\"\n","params = {\n","    \"num_train_epochs\": 3,\n","    \"learning_rate\": 2e-5,\n","    \"per_device_train_batch_size\": 32,\n","    \"per_device_eval_batch_size\": 32,\n","    \"logging_steps\": 100,\n","    \"rank\": 32,\n","    \"lora_alpha\": 16,\n","    \"lora_dropout\": 0.1\n","}\n","\n","mejor_modelo, resultados = sexism_classification_pipeline_task1_LoRA(\n","    SpTrainTask1,\n","    SpDevTask1,\n","    None,\n","    modelname,\n","    2,\n","    \"single_label_classification\",\n","    **params\n",")"],"metadata":{"id":"eyNfp0w4KSvs","executionInfo":{"status":"aborted","timestamp":1747767156365,"user_tz":-120,"elapsed":32244,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2BpaKrUTFzYq","executionInfo":{"status":"aborted","timestamp":1747767156366,"user_tz":-120,"elapsed":32243,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","import torch\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","from sklearn.preprocessing import LabelEncoder\n","\n","# Ruta al tokenizer base y al modelo que guardamos en DRIVE\n","base_model = \"pysentimiento/robertuito-base-uncased\"\n","model_path = \"/content/drive/MyDrive/PRACT4/modelo_final\"\n","\n","# Carga tokenizer y modelo\n","tokenizer = AutoTokenizer.from_pretrained(base_model)\n","model = AutoModelForSequenceClassification.from_pretrained(model_path)\n","\n","# Prepara el dataset de validación\n","label_enc = LabelEncoder()\n","val_dataset = SexismDataset(\n","    SpDevTask1[1],\n","    label_enc.fit_transform(SpDevTask1[2]),\n","    [int(x) for x in SpDevTask1[0]],\n","    tokenizer\n",")\n","\n","# Haz predicción con ambas funciones\n","y_pred1, y_prob1 = predict_op1(model, val_dataset, args={\"per_device_eval_batch_size\": 32})\n","y_pred2, y_prob2 = predict_op2(model, val_dataset, args={\"per_device_eval_batch_size\": 32})\n","\n","# Verifica que ambas salidas coincidan\n","print(\"¿Las predicciones coinciden?\", all(int(a)==int(b) for a, b in zip(y_pred1, y_pred2)))"]},{"cell_type":"markdown","metadata":{"id":"9z5uMJuXFzYr"},"source":["### Evaluation of the results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-1sbIuMFzYr","executionInfo":{"status":"aborted","timestamp":1747767156367,"user_tz":-120,"elapsed":32243,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["preds_es = y_pred1\n","probs_es = y_prob1\n","\n","def compute_metrics(y_true, y_pred):\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        y_true, y_pred, average='binary'\n","    )\n","    acc = accuracy_score(y_true, y_pred)\n","    return {\n","        'accuracy': acc,\n","        'f1': f1,\n","        'precision': precision,\n","        'recall': recall,\n","    }\n","\n","# Calculamos métricas\n","metrics_es = compute_metrics(val_dataset.labels, preds_es)\n","print(\"Métricas (modelo español):\", metrics_es)\n","\n","# Matriz de confusión\n","cm_es = confusion_matrix(val_dataset.labels, preds_es)\n","print(\"\\nMatriz de confusión (modelo español):\")\n","print(cm_es)\n","\n","# Reporte de clasificación\n","# Obtenemos los nombres originales de las clases desde el LabelEncoder\n","target_names_es = label_enc.inverse_transform([0, 1]).tolist()\n","report_es = classification_report(\n","    val_dataset.labels,\n","    preds_es,\n","    target_names=target_names_es,\n","    digits=4\n",")\n","print(\"\\nReporte de clasificación (modelo español):\")\n","print(report_es)\n"]},{"cell_type":"markdown","metadata":{"id":"G7jG9tVpFzYr"},"source":["### Plot confusion matrix\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rk1HnWlsFzYr","executionInfo":{"status":"aborted","timestamp":1747767156367,"user_tz":-120,"elapsed":32242,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","plt.figure(figsize=(6, 4))\n","sns.heatmap(\n","    cm_es,\n","    annot=True,\n","    fmt=\"d\",\n","    cmap=\"Blues\",\n","    cbar=False,\n","    xticklabels=target_names_es,\n","    yticklabels=target_names_es\n",")\n","plt.title(\"Matriz de confusión (modelo español)\")\n","plt.xlabel(\"Predicción\")\n","plt.ylabel(\"Actual\")\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"lzw6YTxSFzYs"},"source":["### Text and probability of False positive and False negative"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"slJxFP7RFzYs","executionInfo":{"status":"aborted","timestamp":1747767156368,"user_tz":-120,"elapsed":32241,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["from typing import List, Tuple\n","\n","def false_positive_false_negative(\n","    y_true: List[int],\n","    y_pred: List[int],\n","    pred_probs: List[float],\n","    texts: List[str],\n",") -> Tuple[List[Tuple[float, str]], List[Tuple[float, str]]]:\n","    \"\"\"\n","    Identifica y retorna las muestras mal clasificadas de un modelo de clasificación binaria.\n","\n","    Parámetros:\n","    - y_true: etiquetas reales (0 o 1).\n","    - y_pred: etiquetas predichas por el modelo (0 o 1).\n","    - pred_probs: probabilidades asociadas a la clase predicha.\n","    - texts: texto de cada muestra.\n","\n","    Retorna:\n","    - false_positives: lista de tuplas (probabilidad, texto) para falsos positivos, ordenada por probabilidad descendente.\n","    - false_negatives: lista de tuplas (probabilidad, texto) para falsos negativos, ordenada por probabilidad descendente.\n","    \"\"\"\n","    # Construir listas con comprensión\n","    false_positives = [\n","        (prob, text)\n","        for true, pred, prob, text in zip(y_true, y_pred, pred_probs, texts)\n","        if true == 0 and pred == 1\n","    ]\n","    false_negatives = [\n","        (prob, text)\n","        for true, pred, prob, text in zip(y_true, y_pred, pred_probs, texts)\n","        if true == 1 and pred == 0\n","    ]\n","\n","    # Ordenar por probabilidad (alta a baja)\n","    false_positives.sort(key=lambda x: x[0], reverse=True)\n","    false_negatives.sort(key=lambda x: x[0], reverse=True)\n","\n","    return false_positives, false_negatives"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mOrcC-QEFzYs","executionInfo":{"status":"aborted","timestamp":1747767156369,"user_tz":-120,"elapsed":32241,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["fpositive, fnegative = false_positive_false_negative(\n","    y_true=val_dataset.labels,\n","    y_pred=preds_es,\n","    pred_probs=probs_es,\n","    texts=val_dataset.texts\n",")\n","\n","print(\"False positive:\", len(fpositive))\n","for s in fpositive:\n","    print('**', s)\n","\n","print(\"\\n\\nFalse negative:\", len(fnegative))\n","for s in fnegative:\n","    print('**', s)\n"]},{"cell_type":"markdown","metadata":{"id":"qs4c9W1HFzYs"},"source":["### Select some samples to analyze\n","\n","You can select the samples with more confidence, those with less confidence, a mix of both, or simply the ones you consider more interesting.\n","\n","\n","\n","**Try not to choose the same samples that I’ve selected, and Justify your decision**.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4W7vZgH9FzYs","executionInfo":{"status":"aborted","timestamp":1747767156370,"user_tz":-120,"elapsed":32240,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["# Make your own selection\n","\n","fpositive_samples = [\n","    fpositive[3][1],\n","    fpositive[10][1],\n","    fpositive[43][1]\n","]\n","fnegative_samples = [\n","    fnegative[1][1],\n","    fnegative[2][1],\n","    fnegative[3][1]\n","]"]},{"cell_type":"code","source":["!pip install ferret-xai"],"metadata":{"id":"dM13gPLqW7EM","executionInfo":{"status":"aborted","timestamp":1747767156371,"user_tz":-120,"elapsed":32240,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tz_42i5HFzYt","executionInfo":{"status":"aborted","timestamp":1747767156372,"user_tz":-120,"elapsed":32239,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["from ferret import Benchmark\n","from IPython.display import display_html\n","\n","# selecting the benchmark\n","bench = Benchmark(model, tokenizer)#, explainers=[s,l])\n","# If you have trouble with this benchmark (runtime errors), try using just those two explainers.\n","    #from ferret.explainers.shap import SHAPExplainer\n","    #from ferret.explainers.lime import LIMEExplainer\n","    #lexp = LIMEExplainer(model, tokenizer)\n","    #sexp = SHAPExplainer(model, tokenizer)\n","    #bench = Benchmark(model, tokenizer, explainers=[sexp,lexp])\n","\n","def explain_this(benchmark, sentence, target):\n","    explanations = benchmark.explain(sentence, target=target)\n","    explanations_de = deduplicate(explanations)\n","    explanation_evaluations = benchmark.evaluate_explanations(explanations_de, target=target)\n","    print(\"Sentence:\", sentence)\n","    print(\"Class:\", target)\n","    tble = benchmark.show_table(explanations_de)\n","    tble2 = benchmark.show_evaluation_table(explanation_evaluations)\n","    display_html(tble.to_html(), raw=True)\n","    display_html(tble2.to_html(), raw=True)"]},{"cell_type":"markdown","metadata":{"id":"6mKPJ_OJFzYt"},"source":["### Show explanations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7BRLQXFNFzYt","executionInfo":{"status":"aborted","timestamp":1747767156373,"user_tz":-120,"elapsed":32239,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["for sample in fpositive_samples:\n","    print(\"False Positive:\")\n","    explain_this(bench, sample, 1)\n","    print(\"\\n\\n\")\n","\n","for sample in fnegative_samples:\n","    print(\"False Negative:\")\n","    explain_this(bench, sample, 0)\n","    print(\"\\n\\n\")"]},{"cell_type":"markdown","metadata":{"id":"ARDG7TNXr3yD","outputId":"1de4ebb2-2e84-4d44-9326-7fe8c6ea341c"},"source":["# DO IT IN ENGLISH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mzUDglkbFzYu","executionInfo":{"status":"aborted","timestamp":1747767156375,"user_tz":-120,"elapsed":32239,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"outputs":[],"source":["# ─────────────────────────────────────────────────────────────────────────────\n","# 0) Montar Google Drive (si quieres persistir ahí)\n","# ─────────────────────────────────────────────────────────────────────────────\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# ─────────────────────────────────────────────────────────────────────────────\n","# 1) Imports y configuración\n","# ─────────────────────────────────────────────────────────────────────────────\n","import os\n","from pathlib import Path\n","\n","import torch\n","from torch.utils.data import Dataset\n","import torch.nn.functional as F\n","\n","import numpy as np\n","import pandas as pd\n","\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    Trainer,\n","    TrainingArguments,\n","    EarlyStoppingCallback\n",")\n","from peft import LoraConfig, get_peft_model, TaskType\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n","from collections import Counter\n","\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","# ─────────────────────────────────────────────────────────────────────────────\n","# 2) Clase de Dataset\n","# ─────────────────────────────────────────────────────────────────────────────\n","class SexismDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=256):\n","        self.texts     = list(texts)\n","        self.labels    = list(labels)\n","        self.tokenizer = tokenizer\n","        self.max_length= max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        enc = self.tokenizer(\n","            self.texts[idx],\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","        item = {k: v.squeeze(0) for k, v in enc.items()}\n","        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return item\n","\n","# ─────────────────────────────────────────────────────────────────────────────\n","# 3) Función de métricas\n","# ─────────────────────────────────────────────────────────────────────────────\n","def compute_metrics_1(pred):\n","    labels = pred.label_ids\n","    preds  = pred.predictions.argmax(-1)\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        labels, preds, average='binary', zero_division=0\n","    )\n","    acc = accuracy_score(labels, preds)\n","    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}\n","\n","# ─────────────────────────────────────────────────────────────────────────────\n","# 4) Pipeline con LoRA y weighted loss\n","# ─────────────────────────────────────────────────────────────────────────────\n","class WeightedTrainer(Trainer):\n","    def __init__(self, *args, class_weights=None, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.class_weights = class_weights\n","\n","    # Ahora acepta kwargs para num_items_in_batch\n","    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n","        labels = inputs.pop(\"labels\")\n","        outputs = model(**inputs)\n","        logits = outputs.logits\n","        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n","        loss = loss_fct(logits, labels)\n","        return (loss, outputs) if return_outputs else loss\n","\n","def sexism_classification_pipeline_task1_LoRA(\n","    trainInfo, devInfo, testInfo=None,\n","    model_name='cardiffnlp/twitter-roberta-base-2022-154m',\n","    nlabels=2,\n","    ptype=\"single_label_classification\",\n","    **args\n","):\n","    # 1) Preparar salida y tokenizer/modelo\n","    output_dir = args.get(\"output_dir\", \"/content/drive/MyDrive/PRACT4/BEST_MODEL_EN\")\n","    Path(output_dir).mkdir(parents=True, exist_ok=True)\n","\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    base_model = AutoModelForSequenceClassification.from_pretrained(\n","        model_name, num_labels=nlabels, problem_type=ptype\n","    )\n","\n","    # 2) Configurar LoRA\n","    lora_config = LoraConfig(\n","        task_type      = args.get(\"task_type\", TaskType.SEQ_CLS),\n","        target_modules = args.get(\"target_modules\", [\"query\", \"key\", \"value\"]),\n","        r              = args.get(\"rank\", 16),\n","        lora_alpha     = args.get(\"lora_alpha\", 32),\n","        lora_dropout   = args.get(\"lora_dropout\", 0.05),\n","        bias           = args.get(\"bias\", \"none\")\n","    )\n","    peft_model = get_peft_model(base_model, lora_config)\n","\n","    # 3) Preparar datos y pesos de clase\n","    train_ids, train_texts, train_labels = trainInfo\n","    dev_ids,   dev_texts,   dev_labels   = devInfo\n","\n","    le = LabelEncoder().fit(train_labels)\n","    y_train = le.transform(train_labels)\n","    y_dev   = le.transform(dev_labels)\n","\n","    freq = Counter(y_train)\n","    weights = torch.tensor([1.0/freq[i] for i in range(nlabels)], dtype=torch.float)\n","    weights = weights / weights.sum()\n","\n","    train_ds = SexismDataset(train_texts, y_train, tokenizer, max_length=args.get(\"max_length\",256))\n","    dev_ds   = SexismDataset(dev_texts,   y_dev,   tokenizer, max_length=args.get(\"max_length\",256))\n","\n","    # 4) Argumentos de entrenamiento\n","    training_args = TrainingArguments(\n","        report_to=\"none\",\n","        output_dir=output_dir,\n","        num_train_epochs=args.get('num_train_epochs', 8),\n","        learning_rate=args.get('learning_rate', 2e-5),\n","        per_device_train_batch_size=args.get('per_device_train_batch_size', 16),\n","        per_device_eval_batch_size=args.get('per_device_eval_batch_size', 16),\n","        fp16=True,\n","        gradient_checkpointing=True,\n","        logging_dir=args.get('logging_dir', './logs'),\n","        logging_steps=args.get('logging_steps', 50),\n","        eval_strategy='epoch',\n","        save_strategy='epoch',\n","        save_total_limit=1,\n","        load_best_model_at_end=True,\n","        metric_for_best_model='f1'\n","    )\n","\n","    trainer = WeightedTrainer(\n","        model=peft_model,\n","        args=training_args,\n","        train_dataset=train_ds,\n","        eval_dataset=dev_ds,\n","        compute_metrics=compute_metrics_1,\n","        callbacks=[EarlyStoppingCallback(early_stopping_patience=args.get(\"early_stopping_patience\",3))],\n","        class_weights=weights\n","    )\n","\n","    # 5) Entrenar y evaluar\n","    trainer.train()\n","    eval_results = trainer.evaluate()\n","    print(\"Validation Results:\", eval_results)\n","\n","    # 6) Merge y guardado en Drive/BEST_MODEL_EN\n","    trainer.save_model(output_dir)              # pesos LoRA\n","    merged = peft_model.merge_and_unload()\n","    merged.save_pretrained(output_dir)          # modelo combinado\n","    tokenizer.save_pretrained(output_dir)       # tokenizer\n","\n","    print(f\"✔ Modelo final guardado en {output_dir}\")\n","    return merged, eval_results\n","\n","# ─────────────────────────────────────────────────────────────────────────────\n","# 5) Ejecutar pipeline\n","# ─────────────────────────────────────────────────────────────────────────────\n","params = {\n","    \"num_train_epochs\":            8,\n","    \"learning_rate\":               2e-5,\n","    \"per_device_train_batch_size\": 16,\n","    \"per_device_eval_batch_size\":  16,\n","    \"logging_steps\":               50,\n","    \"rank\":                        16,\n","    \"lora_alpha\":                  32,\n","    \"lora_dropout\":                0.05,\n","    \"early_stopping_patience\":     3,\n","    \"target_modules\":             [\"query\", \"key\", \"value\"],\n","    \"output_dir\":                 \"/content/drive/MyDrive/PRACT4/BEST_MODEL_EN\",\n","    \"max_length\":                 256\n","}\n","\n","best_model, results = sexism_classification_pipeline_task1_LoRA(\n","    EnTrainTask1,\n","    EnDevTask1,\n","    testInfo=None,\n","    model_name=\"cardiffnlp/twitter-roberta-base-2022-154m\",\n","    nlabels=2,\n","    ptype=\"single_label_classification\",\n","    **params\n",")\n","\n","print(\"Pipeline completado. Métricas:\", results)"]},{"cell_type":"code","source":["# 1) Monta tu Drive (si no lo has hecho aún)\n","\n","# 2) Define dónde está tu modelo merged\n","model_dir = \"/content/drive/MyDrive/PRACT4/BEST_MODEL_EN\"\n","\n","# 3) Carga tokenizer, config y modelo\n","from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_dir)\n","config    = AutoConfig.from_pretrained(model_dir)\n","model     = AutoModelForSequenceClassification.from_pretrained(model_dir, config=config)\n","\n","# 4) Prepara tu Dataset de validación\n","from sklearn.preprocessing import LabelEncoder\n","\n","label_enc = LabelEncoder().fit(EnTrainTask1[2])\n","\n","val_dataset = SexismDataset(\n","    texts     = EnDevTask1[1],\n","    labels    = label_enc.transform(EnDevTask1[2]),\n","    tokenizer = tokenizer,\n","    max_length=256\n",")\n","\n","# 5) Corre la predicción\n","preds, probs = predict_op1(model, val_dataset)\n","\n","# 6) Verifica que ambas funciones coincidan\n","y_pred1, _ = predict_op1(model, val_dataset)\n","y_pred2, _ = predict_op2(model, val_dataset)\n","print(\"¿Predicciones idénticas?\", all(x1 == x2 for x1, x2 in zip(y_pred1, y_pred2)))"],"metadata":{"id":"FNsY6WJYYPsM","executionInfo":{"status":"aborted","timestamp":1747767156376,"user_tz":-120,"elapsed":32239,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import (\n","    precision_recall_fscore_support,\n","    accuracy_score,\n","    confusion_matrix,\n","    classification_report\n",")\n","\n","# 1) Recupera las etiquetas verdaderas (transformadas)\n","#    Asumiendo que `label_enc` ya está entrenado sobre EnTrainTask1[2]\n","y_true = label_enc.transform(EnDevTask1[2])\n","\n","# 2) Tus predicciones (de predict_op1)\n","#    `preds` ya viene como un array de enteros\n","y_pred = preds\n","\n","# 3) Función de métricas\n","def compute_metrics_adj(y_true, y_pred):\n","    precision, recall, f1, _ = precision_recall_fscore_support(\n","        y_true, y_pred, average='binary', zero_division=0\n","    )\n","    acc = accuracy_score(y_true, y_pred)\n","    return {\n","        'accuracy':  acc,\n","        'precision': precision,\n","        'recall':    recall,\n","        'f1':         f1\n","    }\n","\n","metrics = compute_metrics_adj(y_true, y_pred)\n","print(\"Overall metrics:\")\n","for k, v in metrics.items():\n","    print(f\"  {k}: {v:.4f}\")\n","\n","# 4) Matriz de confusión\n","cm = confusion_matrix(y_true, y_pred)\n","print(\"\\nConfusion matrix:\")\n","print(cm)\n","\n","# 5) Reporte de clasificación\n","report = classification_report(\n","    y_true, y_pred,\n","    target_names=label_enc.inverse_transform([0, 1]),\n","    digits=4\n",")\n","print(\"\\nClassification report:\")\n","print(report)\n","\n","# 6) Visualización con heatmap\n","plt.figure(figsize=(6, 5))\n","sns.heatmap(\n","    cm,\n","    annot=True,\n","    fmt=\"d\",\n","    cmap=\"Blues\",\n","    cbar=False,\n","    xticklabels=label_enc.inverse_transform([0, 1]),\n","    yticklabels=label_enc.inverse_transform([0, 1])\n",")\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted label\")\n","plt.ylabel(\"True label\")\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"XMWxNwtbnl5e","executionInfo":{"status":"aborted","timestamp":1747767156377,"user_tz":-120,"elapsed":32239,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from typing import List, Tuple\n","\n","def false_positive_false_negative(\n","    y_true: List[int],\n","    y_pred: List[int],\n","    pred_probs: List[float],\n","    texts: List[str],\n",") -> Tuple[List[Tuple[float, str]], List[Tuple[float, str]]]:\n","    \"\"\"\n","    Identifica y retorna las muestras mal clasificadas de un modelo de clasificación binaria.\n","\n","    Parámetros:\n","    - y_true: etiquetas reales (0 o 1).\n","    - y_pred: etiquetas predichas por el modelo (0 o 1).\n","    - pred_probs: probabilidades asociadas a la clase predicha.\n","    - texts: texto de cada muestra.\n","\n","    Retorna:\n","    - false_positives: lista de tuplas (probabilidad, texto) para falsos positivos, ordenada por probabilidad descendente.\n","    - false_negatives: lista de tuplas (probabilidad, texto) para falsos negativos, ordenada por probabilidad descendente.\n","    \"\"\"\n","    # Construir listas con comprensión\n","    false_positives = [\n","        (prob, text)\n","        for true, pred, prob, text in zip(y_true, y_pred, pred_probs, texts)\n","        if true == 0 and pred == 1\n","    ]\n","    false_negatives = [\n","        (prob, text)\n","        for true, pred, prob, text in zip(y_true, y_pred, pred_probs, texts)\n","        if true == 1 and pred == 0\n","    ]\n","\n","    # Ordenar por probabilidad (alta a baja)\n","    false_positives.sort(key=lambda x: x[0], reverse=True)\n","    false_negatives.sort(key=lambda x: x[0], reverse=True)\n","\n","    return false_positives, false_negatives"],"metadata":{"id":"cffzR71roiAo","executionInfo":{"status":"aborted","timestamp":1747767156387,"user_tz":-120,"elapsed":32247,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1) Ejecuta la predicción\n","preds, probs = predict_op1(model, val_dataset)\n","\n","# 2) Construye la lista pred_probs manejando arrays escalares o vectores\n","pred_probs = []\n","for i, pred in enumerate(preds):\n","    p = probs[i]\n","    if hasattr(p, \"__len__\") and len(p) > 1:\n","        # p es [p(no), p(yes)]\n","        pred_probs.append(float(p[pred]))\n","    else:\n","        # p es escalar = P(yes)\n","        p = float(p)\n","        pred_probs.append(p if pred == 1 else 1.0 - p)\n","\n","# 3) Llama a tu función adaptada\n","fpositive_en, fnegative_en = false_positive_false_negative(\n","    y_true     = val_dataset.labels,   # List[int]\n","    y_pred     = preds.tolist() if isinstance(preds, np.ndarray) else preds,\n","    pred_probs = pred_probs,\n","    texts      = val_dataset.texts\n",")\n","\n","# 4) Imprime resultados\n","print(\"False positive:\", len(fpositive_en))\n","for prob, text in fpositive_en:\n","    print(f\"** [p={prob:.3f}] {text}\")\n","\n","print(\"\\n\\nFalse negative:\", len(fnegative_en))\n","for prob, text in fnegative_en:\n","    print(f\"** [p={prob:.3f}] {text}\")"],"metadata":{"id":"PoWNxn1_nyLn","executionInfo":{"status":"aborted","timestamp":1747767156390,"user_tz":-120,"elapsed":32249,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ferret import Benchmark\n","from IPython.display import display_html\n","\n","# selecting the benchmark\n","bench = Benchmark(model, tokenizer)#, explainers=[s,l])\n","# If you have trouble with this benchmark (runtime errors), try using just those two explainers.\n","    #from ferret.explainers.shap import SHAPExplainer\n","    #from ferret.explainers.lime import LIMEExplainer\n","    #lexp = LIMEExplainer(model, tokenizer)\n","    #sexp = SHAPExplainer(model, tokenizer)\n","    #bench = Benchmark(model, tokenizer, explainers=[sexp,lexp])\n","\n","def explain_this(benchmark, sentence, target):\n","    explanations = benchmark.explain(sentence, target=target)\n","    explanations_de = deduplicate(explanations)\n","    explanation_evaluations = benchmark.evaluate_explanations(explanations_de, target=target)\n","    print(\"Sentence:\", sentence)\n","    print(\"Class:\", target)\n","    tble = benchmark.show_table(explanations_de)\n","    tble2 = benchmark.show_evaluation_table(explanation_evaluations)\n","    display_html(tble.to_html(), raw=True)\n","    display_html(tble2.to_html(), raw=True)"],"metadata":{"id":"Tx-_4-bwqtau","executionInfo":{"status":"aborted","timestamp":1747767156394,"user_tz":-120,"elapsed":32252,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Selecciona solo los ejemplos que quieres explicar\n","fpositive_samples = [fpositive_en[3][1], fpositive_en[17][1], fpositive_en[21][1]]\n","fnegative_samples = [fnegative_en[5][1], fnegative_en[8][1], fnegative_en[14][1]]\n","\n","# Explain para esos falsos positivos concretos\n","for idx, text in zip([3, 17, 21], fpositive_samples):\n","    prob = fpositive_en[idx][0]\n","    print(f\"False Positive sample {idx} [p={prob:.3f}]:\")\n","    explain_this(bench, text, 1)\n","    print(\"\\n\\n\")\n","\n","# Explain para esos falsos negativos concretos\n","for idx, text in zip([5, 8, 14], fnegative_samples):\n","    prob = fnegative_en[idx][0]\n","    print(f\"False Negative sample {idx} [p={prob:.3f}]:\")\n","    explain_this(bench, text, 0)\n","    print(\"\\n\\n\")\n"],"metadata":{"id":"sp64FAovpZNU","executionInfo":{"status":"aborted","timestamp":1747767156396,"user_tz":-120,"elapsed":32252,"user":{"displayName":"Data Scientist","userId":"06164753105265657152"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"}},"nbformat":4,"nbformat_minor":0}