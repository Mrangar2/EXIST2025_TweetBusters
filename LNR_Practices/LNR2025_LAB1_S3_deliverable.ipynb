{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4vFspE6wAC_"
      },
      "source": [
        "<h1 align=\"center\">LAB1_S3. Static word-embeddings for text representation</h1>\n",
        "\n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Natural Language and Information Retrieval</h3>\n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Degree in Data Science</h3>\n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">2024-2025</h3>    \n",
        "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">ETSInf. Universitat PolitÃ¨cnica de ValÃ¨ncia</h3>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Authors:\n",
        "-   Marcos Ranchal\n",
        "-   Marc Siquier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "sE8J46kCONUy"
      },
      "outputs": [],
      "source": [
        "#Installing Gensim library\n",
        "#!pip install -U gensim\n",
        "#!pip install -U nltk\n",
        "#!pip install -U fasttext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4IdboIqwADB"
      },
      "source": [
        "## Some libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4iBKniCGOQ5X",
        "outputId": "8b144cba-8148-4790-a5fc-acd098654c6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\marcos\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#import fasttext.util\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gensim.downloader as downloader\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "\n",
        "nltk.download(\"punkt_tab\")#nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJzPaO1pwADD"
      },
      "source": [
        "## Load both corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "od0PeFAUwADD"
      },
      "outputs": [],
      "source": [
        "path_english = \"EXIST2024_EN_examples.csv\"\n",
        "path_spanish = \"EXIST2024_ES_examples.csv\"\n",
        "\n",
        "df = {\n",
        "    \"english\": pd.read_csv(path_english, sep=\"\\t\"),\n",
        "    \"spanish\": pd.read_csv(path_spanish, sep=\"\\t\")\n",
        "}\n",
        "\n",
        "# OR USING Google colab\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#df = {\n",
        "#    \"english\": pd.read_csv(\"/content/drive/MyDrive/LNR/LNR2025/Lab1/EXIST2024_EN_examples.csv\", sep=\"\\t\"),\n",
        "#    \"spanish\": pd.read_csv(\"/content/drive/MyDrive/LNR/LNR2025/Lab1/EXIST2024_ES_examples.csv\", sep=\"\\t\")\n",
        "#}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koDhhLFYwADE"
      },
      "source": [
        "##Â Preprocess and tokenize the corpora:\n",
        "\n",
        "- remove URLs\n",
        "- remove hashtags\n",
        "- remove users\n",
        "- lowercase\n",
        "- tokenize (using _word_tokenize_)\n",
        "- remove stopwords (using nltk stopwords)\n",
        "\n",
        "Note: tokenization and stopword removal are language-dependent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "WIc7OLcpwADF",
        "outputId": "afd10ecb-aa1e-434e-b3ca-f9ceef2f9ebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hola, mi nombre es Antonio, Â¿todo bien? https://www.upv.es @paquita', 'Hi! my name is Peter']\n",
            "[['hola', ',', 'nombre', 'antonio', ',', 'Â¿todo', 'bien', '?'], ['hi', '!', 'my', 'name', 'is', 'peter']]\n",
            "[['hola', ',', 'mi', 'nombre', 'es', 'antonio', ',', 'Â¿todo', 'bien', '?'], ['hi', '!', 'name', 'peter']]\n"
          ]
        }
      ],
      "source": [
        "# Info: nltk.tokenize.word_tokenize(text, language='english', preserve_line=False)\n",
        "\n",
        "web_re = re.compile(r\"https?:\\/\\/[^\\s]+\", re.U)\n",
        "user_re = re.compile(r\"(@\\w+\\-?(?:\\w+)?)\", re.U)\n",
        "hashtag_re = re.compile(r\"(#\\w+\\-?(?:\\w+)?)\", re.U)\n",
        "\n",
        "stopw = {\n",
        "    \"english\": nltk.corpus.stopwords.words(\"english\"),\n",
        "    \"spanish\": nltk.corpus.stopwords.words(\"spanish\")\n",
        "}\n",
        "\n",
        "def preprocess(text):\n",
        "    # COMPLETE\n",
        "    text = web_re.sub(\"\", text)\n",
        "    text = user_re.sub(\"\", text)\n",
        "    text = hashtag_re.sub(\"\", text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def tokenize(text_list, lang=\"english\"):\n",
        "    #Â COMPLETE\n",
        "    token_list = []\n",
        "    for t in text_list:\n",
        "        text = preprocess(t)\n",
        "        list_t = nltk.tokenize.word_tokenize(text, language=lang, preserve_line=False)\n",
        "        list_t = [word for word in list_t if word not in stopw[lang]]\n",
        "        token_list.append(list_t)\n",
        "    return token_list\n",
        "\n",
        "tokenized_text = {\n",
        "    \"english\": tokenize(df[\"english\"][\"text\"], \"english\"),\n",
        "    \"spanish\": tokenize(df[\"spanish\"][\"text\"], \"spanish\")\n",
        "}\n",
        "\n",
        "t = [\"Hola, mi nombre es Antonio, Â¿todo bien? https://www.upv.es @paquita\", \"Hi! my name is Peter\"]\n",
        "print(t)\n",
        "print(tokenize(t, \"spanish\"))\n",
        "print(tokenize(t, \"english\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUup9ejVwADF"
      },
      "source": [
        "##Â Text representation using static embeddings\n",
        "\n",
        "ENGLISH\n",
        "\n",
        "- word2vec-google-news-300 (using Gemini)\n",
        "- fasttext-wiki-news-subwords-300 (using Gemini)\n",
        "- glove-wiki-gigaword-300 (using Gemini)\n",
        "\n",
        "SPANISH\n",
        "- Fasttext (https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz) (using Gemini)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wClIBsfEwADG"
      },
      "source": [
        "###Â Load the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrrZUm2OwADG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n",
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# COMPLETE\n",
        "import gensim.downloader as download\n",
        "\n",
        "word2vec_model = download.load(\"word2vec-google-news-300\")\n",
        "fasttext_model = download.load(\"fasttext-wiki-news-subwords-300\")\n",
        "glove_model = download.load(\"glove-wiki-gigaword-300\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "es_model = KeyedVectors.load_word2vec_format('cc.es.300.vec.gz', binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load the models from the gensim-data\n",
        "model_path_w2v = r\"C:\\Users\\marcos\\gensim\\word2vec-google-news-300\\word2vec-google-news-300.gz\"\n",
        "model_path_glove = r\"C:\\Users\\marcos\\gensim\\glove-wiki-gigaword-300\\glove-wiki-gigaword-300.gz\"\n",
        "model_path_fasttext = r\"C:\\Users\\marcos\\gensim\\fasttext-wiki-news-subwords-300\\fasttext-wiki-news-subwords-300.gz\"\n",
        "\n",
        "word2vec_model = KeyedVectors.load_word2vec_format(model_path_w2v, binary=True)\n",
        "glove_model = KeyedVectors.load_word2vec_format(model_path_glove, binary=False)\n",
        "fasttext_model = KeyedVectors.load_word2vec_format(model_path_fasttext, binary=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "models_en = {\n",
        "    \"w2v300\": word2vec_model,\n",
        "    \"ftsub300\": fasttext_model,\n",
        "    \"glwiki300\": glove_model}\n",
        "\n",
        "models_es = {\n",
        "    \"ftes300\": es_model\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiVNuQWDwADG"
      },
      "source": [
        "### Compute static word-embeddings representation of the tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOfurEycwADH"
      },
      "outputs": [],
      "source": [
        "# COMPLETE\n",
        "def representacion_oracion_gensim(texto, vectores_clave, dim=300):\n",
        "    palabras = texto\n",
        "    vector_cero = np.zeros(dim)\n",
        "    vector_promedio = np.zeros(dim)\n",
        "    total_palabras = 0\n",
        "    for palabra in palabras:\n",
        "        try:\n",
        "            vector_promedio += vectores_clave[palabra]\n",
        "            total_palabras += 1\n",
        "        except KeyError:\n",
        "            pass\n",
        "    if total_palabras == 0:\n",
        "        return vector_cero\n",
        "    return vector_promedio / total_palabras\n",
        "\n",
        "# Aplicar \"representacion_oracion_gensim\"\n",
        "df[\"english\"][\"w2v300\"] = [representacion_oracion_gensim(tweet, word2vec_model) for tweet in tokenized_text[\"english\"]]\n",
        "df[\"english\"][\"ftsub300\"] = [representacion_oracion_gensim(tweet, fasttext_model) for tweet in tokenized_text[\"english\"]]\n",
        "df[\"english\"][\"glwiki300\"] = [representacion_oracion_gensim(tweet, glove_model) for tweet in tokenized_text[\"english\"]]\n",
        "df[\"spanish\"][\"ftes300\"] = [representacion_oracion_gensim(tweet, es_model) for tweet in tokenized_text[\"spanish\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RautX5QvwADH"
      },
      "source": [
        "##Â Compute cosine similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkBr3hzkwADH"
      },
      "outputs": [],
      "source": [
        "# COMPLETE\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def encontrar_mas_similar(vectores_texto, etiquetas, etiqueta_objetivo, modelo, idioma=\"english\"):\n",
        "    indices = df[idioma][etiquetas] == etiqueta_objetivo\n",
        "    sub_vectores = np.stack(df[idioma][indices][modelo].values)\n",
        "    similitud_coseno = cosine_similarity(sub_vectores)\n",
        "\n",
        "    max_similitud, mejor_par = 0, (None, None)\n",
        "    for i in range(sub_vectores.shape[0]): \n",
        "        for j in range(i + 1, sub_vectores.shape[0]):\n",
        "            if similitud_coseno[i, j] > max_similitud:\n",
        "                max_similitud = similitud_coseno[i, j]\n",
        "                mejor_par = (df[idioma][indices].iloc[i][\"id\"], df[idioma][indices].iloc[j][\"id\"])\n",
        "\n",
        "    return mejor_par, max_similitud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4EDMZpmwADH"
      },
      "source": [
        "## Show results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twmmF2qfwADI",
        "outputId": "48501832-7552-446a-bf70-fc0b8b18aed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================\n",
            "w2v300 \n",
            "----------------------\n",
            "Label: NO \n",
            "Tweets IDs: (201173, 201177) \n",
            "Similarity: 0.9444\n",
            "Tweets: \n",
            " \t1: @BLEEDTHISWAY replay free woman breebylon &gt;&gt;&gt; Flop this way \n",
            " \t2: replay&gt;alice&gt;babylon&gt;free woman https://t.co/WCEqeUxdtC\n",
            "----------------------\n",
            "Label: YES \n",
            "Tweets IDs: (201621, 201637) \n",
            "Similarity: 0.9614\n",
            "Tweets: \n",
            " \t1: @WeaponizedRage Aerosmith in 1987: \"Dude looks like a lady\" \n",
            " \t2: Dude does not look like a lady! https://t.co/C62JmKSzy0\n",
            "======================\n",
            "ftsub300 \n",
            "----------------------\n",
            "Label: NO \n",
            "Tweets IDs: (201173, 201177) \n",
            "Similarity: 0.9898\n",
            "Tweets: \n",
            " \t1: @BLEEDTHISWAY replay free woman breebylon &gt;&gt;&gt; Flop this way \n",
            " \t2: replay&gt;alice&gt;babylon&gt;free woman https://t.co/WCEqeUxdtC\n",
            "----------------------\n",
            "Label: YES \n",
            "Tweets IDs: (201235, 201978) \n",
            "Similarity: 0.9716\n",
            "Tweets: \n",
            " \t1: in the living room, all strocking to porn. all4guys+host had fucked me. 1st, straight,had cum in cumhole. I was so surprised&amp; pleased.even if short &amp;light, was so hot be gangbanged w/o knowing, blindfolded, w straight men &amp;bred by straight man!A108M31 #chastity #cumdump #gangbang \n",
            " \t2: @TabitaSurge Ha! My English teacher did similar in year 7 when I commented \"b/c the patriarchy was, still is &amp; always will be, bullshit\" in discussion about Portia/ Merchant of Venice. She loaned me books &amp; it was through her I discovered Wollstonecraft, Dworkin, Butler, Lorde and more...\n",
            "======================\n",
            "glwiki300 \n",
            "----------------------\n",
            "Label: NO \n",
            "Tweets IDs: (201173, 201177) \n",
            "Similarity: 0.9690\n",
            "Tweets: \n",
            " \t1: @BLEEDTHISWAY replay free woman breebylon &gt;&gt;&gt; Flop this way \n",
            " \t2: replay&gt;alice&gt;babylon&gt;free woman https://t.co/WCEqeUxdtC\n",
            "----------------------\n",
            "Label: YES \n",
            "Tweets IDs: (200204, 200217) \n",
            "Similarity: 0.9565\n",
            "Tweets: \n",
            " \t1: @lkmeenha we canâ€™t even have a day without women making it about themselves ðŸ™„ \n",
            " \t2: @BigDILF01 Canâ€™t go a day without women womening\n",
            "======================\n",
            "ftes300 \n",
            "----------------------\n",
            "Label: NO \n",
            "Tweets IDs: (102216, 102218) \n",
            "Similarity: 1.0000\n",
            "Tweets: \n",
            " \t1: ðŸŒµ Hacer luz de gas: todo sobre el maltrato psicolÃ³gico que estÃ¡s sufriendo sin saberlo... https://t.co/u7CmhfpcWS \n",
            " \t2: ðŸŽ¯ Hacer luz de gas: todo sobre el maltrato psicolÃ³gico que estÃ¡s sufriendo sin saberlo... https://t.co/momOrNWO7S\n",
            "----------------------\n",
            "Label: YES \n",
            "Tweets IDs: (102508, 102517) \n",
            "Similarity: 1.0000\n",
            "Tweets: \n",
            " \t1: Mujer al volante, peligro constante https://t.co/lgIKAgt9yR \n",
            " \t2: @balin2522 @julianaoxenford @aliciaretto @MTC_GobPeru Mujer al volante, peligro constante  ðŸ˜…ðŸ˜…ðŸ˜…ðŸš—ðŸš—ðŸš—\n"
          ]
        }
      ],
      "source": [
        "# COMPLETE\n",
        "\n",
        "for name, vectors in models_en.items():\n",
        "    print(f\"======================\\n{name}\\n\" + \"-\" * 22)\n",
        "\n",
        "    for label in ['NO', 'YES']:\n",
        "        best_pair, similarity = encontrar_mas_similar(vectors, \"label\", label, name, lang=\"english\")\n",
        "        print(f\"Label: {label} \\nTweets IDs: {best_pair} \\nSimilarity: {similarity:.4f}\")\n",
        "        print(f\"Tweets: \\n \\t1: {df[\"english\"][df[\"english\"]['id'] == best_pair[0]]['text'].values[0]} \\n \\t2: {df[\"english\"][df[\"english\"]['id'] == best_pair[1]]['text'].values[0]}\")\n",
        "        if label == \"NO\":\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "\n",
        "for name, vectors in models_es.items():\n",
        "    print(f\"======================\\n{name}\\n\" + \"-\" * 22)\n",
        "    for label in ['NO', 'YES']:\n",
        "        best_pair, similarity = encontrar_mas_similar(vectors, \"label\", label, name, lang=\"spanish\")\n",
        "        print(f\"Label: {label} \\nTweets IDs: {best_pair} \\nSimilarity: {similarity:.4f}\")\n",
        "        print(f\"Tweets: \\n \\t1: {df['spanish'][df['spanish']['id'] == best_pair[0]]['text'].values[0]} \\n \\t2: {df['spanish'][df['spanish']['id'] == best_pair[1]]['text'].values[0]}\")\n",
        "        if label == \"NO\":\n",
        "            print(\"-\" * 20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
