{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Lab 2:  Sexism Identification in Twitter</h1>\n",
    "<h2 align=\"center\">Session 1. Machine Learning and Feature Engineering</h2>\n",
    "\n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Natural Language and Information Retrieval</h3>\n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">Degree in Data Science</h3>\n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">2024-2025</h3>    \n",
    "<h3 style=\"display:block; margin-top:5px;\" align=\"center\">ETSInf. Universitat Politècnica de València</h3>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put your names here\n",
    "\n",
    "- Marcos Ranchal\n",
    "- Marc Siquier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "id": "Uexir0NmwwBi"
   },
   "outputs": [],
   "source": [
    "# Reading the entire dataset for both languages and considering only the hard labels. In this lab we do not address the sexism identification task from a Learning with Disagreement (LwD) perspective.\n",
    "\n",
    "from readerEXIST2025 import EXISTReader\n",
    "\n",
    "reader_train = EXISTReader(\"EXIST_2025_Dataset_V0.2/EXIST2025_training.json\")\n",
    "reader_dev = EXISTReader(\"EXIST_2025_Dataset_V0.2/EXIST2025_dev.json\")\n",
    "\n",
    "EnTrainTask1, EnDevTask1 = reader_train.get(lang=\"EN\", subtask=\"1\"), reader_dev.get(lang=\"EN\", subtask=\"1\")\n",
    "EnTrainTask2, EnDevTask2 = reader_train.get(lang=\"EN\", subtask=\"2\"), reader_dev.get(lang=\"EN\", subtask=\"2\")\n",
    "\n",
    "SpTrainTask1, SpDevTask1 = reader_train.get(lang=\"ES\", subtask=\"1\"), reader_dev.get(lang=\"ES\", subtask=\"1\")\n",
    "SpTrainTask2, SpDevTask2 = reader_train.get(lang=\"ES\", subtask=\"2\"), reader_dev.get(lang=\"ES\", subtask=\"2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voy a ir explicando el proceso paso a paso para que puedas replicarlo, luego cuando lo tengamos lo arreglamos y traducimos y ponemos alguna conclusión final o algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENGLISH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE IF YOU WANT TO DO TEXT PREPROCESSING\n",
    "import nltk\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizer, BertModel, RobertaTokenizer, RobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import emoji\n",
    "\n",
    "web_re = re.compile(r\"https?:\\/\\/[^\\s]+\", re.U)\n",
    "user_re = re.compile(r\"(@\\w+\\-?(?:\\w+)?)\", re.U)\n",
    "hashtag_re = re.compile(r\"(#\\w+\\-?(?:\\w+)?)\", re.U)\n",
    "emojis = sorted(emoji.EMOJI_DATA, key=len, reverse=True)\n",
    "pattern_emoji = '(' + '|'.join(re.escape(u) for u in emojis) + ')'\n",
    "symbols=r\"[()'\\\"¿?!¡;,:.?\\.]\"\n",
    "decimal = r\"(\\d+[.,]\\d+(?:[%]?)?)\"\n",
    "dates = r\"(\\d{1,2}[/\\-]\\d{1,2}(?:[/\\-]\\d{2,4})?)\"\n",
    "hora = r\"\\b\\d{1,2}:\\d{2}(?::\\d{2})?\\s?(?:[ap]\\.?m\\.?)?\"\n",
    "fechas_format = r\"(\\d{1,2}\\s+\\w+\\s+\\d{4})\"\n",
    "\n",
    "\n",
    "stopw = {\n",
    "    \"english\": nltk.corpus.stopwords.words(\"english\"),\n",
    "    \"spanish\": nltk.corpus.stopwords.words(\"spanish\")\n",
    "}\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    # COMPLETE\n",
    "    text = web_re.sub(\"\", text)\n",
    "    text = user_re.sub(\"\", text)\n",
    "    text = hashtag_re.sub(\"\", text)\n",
    "    text = re.sub(pattern_emoji, \"\", text)\n",
    "    text = re.sub(symbols, \"\", text)\n",
    "    text = re.sub(dates, \"\", text)\n",
    "    text  = re.sub(decimal, \"\", text)\n",
    "    text  = re.sub(hora, \"\", text)\n",
    "    text = re.sub(fechas_format, \"\", text)\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def tokenize(text_list, lang=\"english\"):\n",
    "    # COMPLETE\n",
    "    token_list = []\n",
    "    for t in text_list:\n",
    "        text = preprocess(t)\n",
    "        list_t = nltk.tokenize.word_tokenize(text, language=lang, preserve_line=False)\n",
    "        list_t = [word for word in list_t if word not in stopw[lang]]\n",
    "        token_list.append(\" \".join(list_t))\n",
    "    return token_list\n",
    "\n",
    "tokenized_text = {\n",
    "    \"english_train_1\": tokenize(EnTrainTask1[1], \"english\"),\n",
    "    \"english_test_1\": tokenize(EnDevTask1[1], \"english\"),\n",
    "    \"english_train_2\": tokenize(EnTrainTask2[1], \"english\"),\n",
    "    \"english_test_2\": tokenize(EnDevTask2[1], \"english\")\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, aplicamos limpieza de texto utilizando los patrones re de la sesión 1 del laboratorio, para eliminar elementos extraños.\n",
    "\n",
    "Además, detectamos los stop words con nltk y los eliminamos de cada tweet para que no afecten.\n",
    "\n",
    "Vemos el resultado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing uni essay local pub coffee random old man keeps asking drunk questions im trying concentrate & amp ends good luck youll end getting married use anyway alive well\n",
      "20 dont appreciate two rides team member looked behind asked man behind many party impressed\n",
      "according customer plenty time go spent stirling coins wants pay derry like woman im sure retail\n",
      "blokes drink beer sorry arent bloke drink wine apparently alive well\n",
      "new shelves week - looking forward reading books\n"
     ]
    }
   ],
   "source": [
    "for i in range(min(5, len(tokenized_text[\"english_train_1\"]))):\n",
    "    print(tokenized_text[\"english_train_1\"][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet representations (Feature extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se hace lo siguiente:\n",
    "1. Convertir la columna 2 de los datasets (etiquetas) a listas.\n",
    "2. Dividir cada train con train_test_split (no sería necesario realmente).\n",
    "3. Se carga un SVD (n=100) para reducir la dimensionalidad, un vectorizer basado en Term Frequency y un Label Encoder para codificar las etiquetas (one hot encoding).\n",
    "4. Se convierten los vectores de entrenamiento y de test (primero pasan por el vectorizer, y luego por el SVD para reducir las dimensiones).\n",
    "5. A las Y's se le aplica el Label Encoder.\n",
    "\n",
    "NOTA: Es importante que se utilicen instancias diferentes de cada modelo para las transformaciones en la task1 y en la task2, pues task1 es binaria y task2 multiclase (si no da errores).\n",
    "\n",
    "Esta representación es estática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y = EnTrainTask1[2].tolist()\n",
    "X_train, y_train = tokenized_text[\"english_train_1\"], y\n",
    "\n",
    "\n",
    "svd1 = TruncatedSVD(n_components=100)\n",
    "\n",
    "vectorizer_task1 = TfidfVectorizer(norm=\"l2\")\n",
    "label_encoder_task1 = LabelEncoder()\n",
    "\n",
    "X_train_vectors = vectorizer_task1.fit_transform(X_train)\n",
    "lsa_vectors_train_1 = svd1.fit_transform(X_train_vectors)\n",
    "\n",
    "\n",
    "\n",
    "y_train_numeric = label_encoder_task1.fit_transform(y_train)\n",
    "\n",
    "\n",
    "y2 = EnTrainTask2[2].tolist()\n",
    "X_train_2,  y_train_2 = tokenized_text[\"english_train_2\"], y2\n",
    "\n",
    "vectorizer_task2 = TfidfVectorizer(norm=\"l2\")\n",
    "label_encoder_task2 = LabelEncoder()\n",
    "svd2 = TruncatedSVD(n_components=100)\n",
    "\n",
    "X_train_vectors_2 = vectorizer_task2.fit_transform(X_train_2)\n",
    "lsa_vectors_train_2 = svd2.fit_transform(X_train_vectors_2)\n",
    "\n",
    "\n",
    "\n",
    "y_train_numeric_2 = label_encoder_task2.fit_transform(y_train_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificamos las dimensiones de los lsa_vectors, en todo caso deben tener 100 columnas cada uno por la reducción de dimensionalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones de lsa_vectors_train_1: (2870, 100)\n",
      "Dimensiones de lsa_vectors_train_2: (856, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensiones de lsa_vectors_train_1:\", lsa_vectors_train_1.shape)\n",
    "print(\"Dimensiones de lsa_vectors_train_2:\", lsa_vectors_train_2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquí se aplican las mismas transformaciones que se han hecho sobre los datos de entrenamiento sobre los datos de test (Dev). Hay que asegurarse de que los vectorizers, SVD y label encoders son los mismos que antes en su respectiva tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_test_X_task1 = tokenized_text[\"english_test_1\"]\n",
    "X_test_vectors_task1 = vectorizer_task1.fit_transform(real_test_X_task1)\n",
    "lsa_vectors_test_task1 = svd1.fit_transform(X_test_vectors_task1)\n",
    "\n",
    "real_test_y_task1 = EnDevTask1[2].tolist()\n",
    "y_test_task1 = label_encoder_task1.fit_transform(real_test_y_task1)\n",
    "\n",
    "\n",
    "\n",
    "real_test_X_task2 = tokenized_text[\"english_test_2\"]\n",
    "X_test_vectors_task2 = vectorizer_task2.fit_transform(real_test_X_task2)\n",
    "lsa_vectors_test_task2 = svd2.fit_transform(X_test_vectors_task2)\n",
    "\n",
    "real_test_y_task2 = EnDevTask2[2].tolist()\n",
    "y_test_task2 = label_encoder_task2.fit_transform(real_test_y_task2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos nuevamente la dimensionalidad, pero ahora de los vectores de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(444, 100)\n",
      "(146, 100)\n"
     ]
    }
   ],
   "source": [
    "print(lsa_vectors_test_task1.shape)\n",
    "print(lsa_vectors_test_task2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se obtienen los embeddings contextuales con BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at PlanTL-GOB-ES/roberta-base-bne and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: roberta-base, torch.Size([2870, 768])\n",
      "Model: roberta-base, torch.Size([856, 768])\n",
      "Model: roberta-base, torch.Size([444, 768])\n",
      "Model: roberta-base, torch.Size([146, 768])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():  # Mac M? GPU\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():  # Nvidia GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "else:  # CPU\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "modelnames = {\"english\": [\"roberta-base\"], \"spanish\": [\"PlanTL-GOB-ES/roberta-base-bne\"]}   \n",
    "\n",
    "tokenizers = {\"english\": [], \"spanish\": []}\n",
    "models = {\"english\": [], \"spanish\": []}\n",
    "\n",
    "for lang in [\"english\", \"spanish\"]:\n",
    "    for modelname in modelnames[lang]:\n",
    "            tokenizer = RobertaTokenizer.from_pretrained(modelname)\n",
    "            model = RobertaModel.from_pretrained(modelname)\n",
    "            tokenizers[lang].append(tokenizer)\n",
    "            models[lang].append(model)\n",
    "        \n",
    "batch_size = 16 \n",
    "\n",
    "# COMPLETE\n",
    "\n",
    "\n",
    "vectors = {\"task1\": {\"english\": {\"train\":{}, \"test\":{}}, \"spanish\": {\"train\":{}, \"test\":{}}}, \"task2\":{\"english\": {\"train\":{}, \"test\":{}}, \"spanish\": {\"train\":{}, \"test\":{}}}}\n",
    "\n",
    "\n",
    "tweetsTotal = [tokenized_text[\"english_train_1\"], tokenized_text[\"english_train_2\"], tokenized_text[\"english_test_1\"], tokenized_text[\"english_test_2\"]]\n",
    "\n",
    "task = 1\n",
    "\n",
    "lang = \"english\"\n",
    "\n",
    "for tweets in tweetsTotal:\n",
    "    for i, model in enumerate(models[lang]):\n",
    "        tokenizer = tokenizers[lang][i]\n",
    "        model = models[lang][i]\n",
    "        name = modelnames[lang][i]\n",
    "\n",
    "\n",
    "        tensor_list=[]\n",
    "\n",
    "\n",
    "        for i in range(0, len(tweets), batch_size):\n",
    "            batch = tweets[i:i+batch_size]\n",
    "            input = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            model.eval()\n",
    "            model.to(device)\n",
    "            input = input.to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**input)\n",
    "                # Transformers models always return tuples.\n",
    "                # Here, the first element corresponds to the vectors in the output of the last BETO layer.\n",
    "                encoded_layers = outputs[0]\n",
    "                #Here we obtain the embedding of the CLS tokens for each input text.\n",
    "                #This representation serves as a contextual embedding of the texts.\n",
    "                cls_vector = encoded_layers[:,0,:]\n",
    "                #Vector associated with the CLS token of the first text in the entry.\n",
    "            tensor_list.append(cls_vector)\n",
    "        cls_vector = torch.cat(tensor_list).cpu()#.detach().numpy()[0]\n",
    "        if task == 1:\n",
    "            vectors[\"task1\"][lang][\"train\"][name] = cls_vector\n",
    "        elif task == 2:\n",
    "            vectors[\"task2\"][lang][\"train\"][name] = cls_vector\n",
    "        elif task == 3:\n",
    "            vectors[\"task1\"][lang][\"test\"][name] = cls_vector\n",
    "        else:\n",
    "            vectors[\"task2\"][lang][\"test\"][name] = cls_vector\n",
    "\n",
    "        task += 1\n",
    "        print(f\"Model: {name}, {(cls_vector.size())}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos un ejemplo de los vectores de entrenamiento para ver que son tensores de las dimensiones adecuadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0444,  0.0395, -0.0468,  ..., -0.1237, -0.0360,  0.0144],\n",
       "        [-0.0409,  0.0366, -0.0349,  ..., -0.0743, -0.0789, -0.0020],\n",
       "        [-0.0126,  0.0489, -0.0073,  ..., -0.1055, -0.0543, -0.0028],\n",
       "        ...,\n",
       "        [-0.0002,  0.0236, -0.0423,  ..., -0.1465, -0.0481,  0.0078],\n",
       "        [-0.0559,  0.0523, -0.0239,  ..., -0.1038, -0.0295, -0.0287],\n",
       "        [-0.0385,  0.0478, -0.0233,  ..., -0.1351, -0.0708, -0.0144]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[\"task1\"][\"english\"][\"train\"][\"roberta-base\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se entrenan los modelos.\n",
    "\n",
    "1. MLP\n",
    "2. XGBoost\n",
    "\n",
    "Se usan los vectores de train, lsa_vectors_train, en el caso de static embeddings, y vectors[\"task1\"][\"english\"][\"train\"][\"roberta-base\"], en el caso de contextual embeddings. Se consiguen al final un total de 2 (modelos) x 2 (métodos de embeddings) x 2 (tasks) = 8 predicciones finales.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos tarea 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marcos\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\marcos\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### STATIC\n",
    "\n",
    "clf1_en_1_static = SVC(random_state=1, max_iter=1000)\n",
    "clf1_en_1_static.fit(lsa_vectors_train_1, y_train_numeric)\n",
    "\n",
    "# Convert data to DMatrix format\n",
    "dtrain1_en_static = xgb.DMatrix(lsa_vectors_train_1, label=y_train_numeric)\n",
    "dtest1_en_static = xgb.DMatrix(lsa_vectors_test_task1, label=y_test_task1)\n",
    "# Set XGBoost parameters\n",
    "params_static = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc'\n",
    "}\n",
    "# Train the model\n",
    "num_rounds_static = 100\n",
    "bst_en_1_static = xgb.train(params_static, dtrain1_en_static, num_rounds_static)\n",
    "\n",
    "\n",
    "\n",
    "### CONTEXTUAL\n",
    "\n",
    "clf1_en_1_contextual = SVC(random_state=1, max_iter=1000)\n",
    "clf1_en_1_contextual.fit(vectors[\"task1\"][\"english\"][\"train\"][\"roberta-base\"], y_train_numeric)\n",
    "\n",
    "\n",
    "# Convert data to DMatrix format\n",
    "dtrain1_en_contextual = xgb.DMatrix(vectors[\"task1\"][\"english\"][\"train\"][\"roberta-base\"], label=y_train_numeric)\n",
    "dtest1_en_contextual = xgb.DMatrix(vectors[\"task1\"][\"english\"][\"test\"][\"roberta-base\"], label=y_test_task1)\n",
    "# Set XGBoost parameters\n",
    "params_contextual = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc'\n",
    "}\n",
    "# Train the model\n",
    "num_rounds_contextual = 100\n",
    "bst_en_1_contextual = xgb.train(params_contextual, dtrain1_en_contextual, num_rounds_contextual)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos tarea 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1_en_2_static = SVC(random_state=1, max_iter=1000)\n",
    "clf1_en_2_static.fit(lsa_vectors_train_2, y_train_numeric_2)\n",
    "\n",
    "# Convert data to DMatrix format\n",
    "dtrain2_en_static = xgb.DMatrix(lsa_vectors_train_2, label=y_train_numeric_2)\n",
    "dtest2_en_static = xgb.DMatrix(lsa_vectors_test_task2, label=y_test_task2)\n",
    "# Set XGBoost parameters\n",
    "params_static = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'multi:softmax',  # Change objective to multi:softmax for multi-class classification\n",
    "    'num_class': 3,  # Specify the number of classes\n",
    "    'eval_metric': 'mlogloss'\n",
    "}\n",
    "# Train the model\n",
    "num_rounds_static = 100\n",
    "bst_en_2_static = xgb.train(params_static, dtrain2_en_static, num_rounds_static)\n",
    "\n",
    "\n",
    "\n",
    "### CONTEXTUAL\n",
    "\n",
    "clf1_en_2_contextual = SVC(random_state=1, max_iter=1000)\n",
    "clf1_en_2_contextual.fit(vectors[\"task2\"][\"english\"][\"train\"][\"roberta-base\"], y_train_numeric_2)\n",
    "\n",
    "# Convert data to DMatrix format\n",
    "dtrain2_en_contextual = xgb.DMatrix(vectors[\"task2\"][\"english\"][\"train\"][\"roberta-base\"], label=y_train_numeric_2)\n",
    "dtest2_en_contextual = xgb.DMatrix(vectors[\"task2\"][\"english\"][\"test\"][\"roberta-base\"], label=y_test_task2)\n",
    "# Set XGBoost parameters\n",
    "params_contextual = {\n",
    "    'max_depth': 3,\n",
    "    'eta': 0.1,\n",
    "    'objective': 'multi:softmax',  # Change objective to multi:softmax for multi-class classification\n",
    "    'num_class': 3,  # Specify the number of classes\n",
    "    'eval_metric': 'mlogloss'\n",
    "}\n",
    "# Train the model\n",
    "num_rounds_contextual = 100\n",
    "bst_en_2_contextual = xgb.train(params_contextual, dtrain2_en_contextual, num_rounds_contextual)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente realizamos predict sobre cada modelo con los lsa_vectors_test_taskX, que se corresponden con los vectores del dataset de test (Dev). Volvemos a generar la matriz XGB para el modelo XGBoost utilizando los datos Dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predicted1_1_static = clf1_en_1_static.predict(lsa_vectors_test_task1)\n",
    "\n",
    "predicted1_1_contextual = clf1_en_1_contextual.predict(vectors[\"task1\"][\"english\"][\"test\"][\"roberta-base\"])\n",
    "\n",
    "# Make predictions on the test set\n",
    "predicted1_2_static = bst_en_1_static.predict(dtest1_en_static)\n",
    "predicted1_2_static = np.asarray([int(round(value)) for value in predicted1_2_static])\n",
    "\n",
    "# Make predictions on the test set\n",
    "predicted1_2_contextual = bst_en_1_contextual.predict(dtest1_en_contextual)\n",
    "predicted1_2_contextual = np.asarray([int(round(value)) for value in predicted1_2_contextual])\n",
    "\n",
    "# Generar predicciones para EnDevTask2 usando los modelos _2\n",
    "dtest2_en = xgb.DMatrix(lsa_vectors_test_task2, label=y_test_task2)\n",
    "\n",
    "predicted2_1_static = clf1_en_2_static.predict(lsa_vectors_test_task2)\n",
    "\n",
    "predicted2_1_contextual = clf1_en_2_contextual.predict(vectors[\"task2\"][\"english\"][\"test\"][\"roberta-base\"])\n",
    "\n",
    "# Make predictions on the test set\n",
    "predicted2_2_static = bst_en_2_static.predict(dtest2_en_static)\n",
    "predicted2_2_static = np.asarray([int(round(value)) for value in predicted2_2_static])\n",
    "\n",
    "# Make predictions on the test set\n",
    "predicted2_2_contextual = bst_en_2_contextual.predict(dtest2_en_contextual)\n",
    "predicted2_2_contextual = np.asarray([int(round(value)) for value in predicted2_2_contextual])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos los resultados de cada modelo para cada tarea. Utilizamos la función del profesor show_subtaskX, que recibe de argumentos ref (y del dataset Dev de la tarea) y pred (las predicciones finales de nuestros modelos a partir de las X del dataset Dev de la tarea)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENGLISH\n",
      "\n",
      "STATIC EMBEDDINGS: \n",
      "\n",
      "SUBTASK 1\n",
      "\n",
      "=================================\n",
      "\n",
      "MLP: \n",
      "\n",
      "SUBTASK 1\n",
      "F1-score (Positive Class): 0.5876288659793815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6800    0.6800    0.6800       250\n",
      "           1     0.5876    0.5876    0.5876       194\n",
      "\n",
      "    accuracy                         0.6396       444\n",
      "   macro avg     0.6338    0.6338    0.6338       444\n",
      "weighted avg     0.6396    0.6396    0.6396       444\n",
      "\n",
      "=================================\n",
      "\n",
      "XGBoost: \n",
      "\n",
      "SUBTASK 1\n",
      "F1-score (Positive Class): 0.6142131979695431\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7008    0.6840    0.6923       250\n",
      "           1     0.6050    0.6237    0.6142       194\n",
      "\n",
      "    accuracy                         0.6577       444\n",
      "   macro avg     0.6529    0.6539    0.6533       444\n",
      "weighted avg     0.6590    0.6577    0.6582       444\n",
      "\n",
      "=================================\n",
      "\n",
      "SUBTASK 2\n",
      "\n",
      "=================================\n",
      "\n",
      "MLP: \n",
      "\n",
      "SUBTASK 2\n",
      "F1-score (Macro-Averaged): 0.24531024531024528\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5822    1.0000    0.7359        85\n",
      "           1     0.0000    0.0000    0.0000        28\n",
      "           2     0.0000    0.0000    0.0000        33\n",
      "\n",
      "    accuracy                         0.5822       146\n",
      "   macro avg     0.1941    0.3333    0.2453       146\n",
      "weighted avg     0.3389    0.5822    0.4285       146\n",
      "\n",
      "=================================\n",
      "\n",
      "XGBoost: \n",
      "\n",
      "SUBTASK 2\n",
      "F1-score (Macro-Averaged): 0.22822822822822822\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5547    0.8941    0.6847        85\n",
      "           1     0.0000    0.0000    0.0000        28\n",
      "           2     0.0000    0.0000    0.0000        33\n",
      "\n",
      "    accuracy                         0.5205       146\n",
      "   macro avg     0.1849    0.2980    0.2282       146\n",
      "weighted avg     0.3230    0.5205    0.3986       146\n",
      "\n",
      "=================================\n",
      "\n",
      "ENGLISH\n",
      "\n",
      "CONTEXTUAL EMBEDDINGS: \n",
      "\n",
      "SUBTASK 1\n",
      "\n",
      "=================================\n",
      "\n",
      "MLP: \n",
      "\n",
      "SUBTASK 1\n",
      "F1-score (Positive Class): 0.6631762652705061\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9385    0.2440    0.3873       250\n",
      "           1     0.5013    0.9794    0.6632       194\n",
      "\n",
      "    accuracy                         0.5653       444\n",
      "   macro avg     0.7199    0.6117    0.5252       444\n",
      "weighted avg     0.7475    0.5653    0.5078       444\n",
      "\n",
      "=================================\n",
      "\n",
      "XGBoost: \n",
      "\n",
      "SUBTASK 1\n",
      "F1-score (Positive Class): 0.7382920110192838\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7818    0.8600    0.8190       250\n",
      "           1     0.7929    0.6907    0.7383       194\n",
      "\n",
      "    accuracy                         0.7860       444\n",
      "   macro avg     0.7874    0.7754    0.7787       444\n",
      "weighted avg     0.7867    0.7860    0.7838       444\n",
      "\n",
      "=================================\n",
      "\n",
      "SUBTASK 2\n",
      "\n",
      "=================================\n",
      "\n",
      "MLP: \n",
      "\n",
      "SUBTASK 2\n",
      "F1-score (Macro-Averaged): 0.24531024531024528\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5822    1.0000    0.7359        85\n",
      "           1     0.0000    0.0000    0.0000        28\n",
      "           2     0.0000    0.0000    0.0000        33\n",
      "\n",
      "    accuracy                         0.5822       146\n",
      "   macro avg     0.1941    0.3333    0.2453       146\n",
      "weighted avg     0.3389    0.5822    0.4285       146\n",
      "\n",
      "=================================\n",
      "\n",
      "XGBoost: \n",
      "\n",
      "SUBTASK 2\n",
      "F1-score (Macro-Averaged): 0.5153535353535353\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6903    0.9176    0.7879        85\n",
      "           1     0.4375    0.2500    0.3182        28\n",
      "           2     0.6471    0.3333    0.4400        33\n",
      "\n",
      "    accuracy                         0.6575       146\n",
      "   macro avg     0.5916    0.5003    0.5154       146\n",
      "weighted avg     0.6320    0.6575    0.6192       146\n",
      "\n",
      "=================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#EXAMPLE 1 TASK1 and TASK2 FOR ENGLISH\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "def show_subtask1(ref, pred):\n",
    "    # Computing the F1 Score for the positive class (sexism), subtask 1\n",
    "    f1_positive = f1_score(ref, pred, pos_label=1, zero_division=0)\n",
    "    # Obtainig a detailed classification report\n",
    "    report = classification_report(ref, pred, digits=4, zero_division=0)   \n",
    "    print(\"\\nSUBTASK 1\")\n",
    "    print(f\"F1-score (Positive Class): {f1_positive}\")\n",
    "    print(report)\n",
    "\n",
    "def show_subtask2(ref, pred):\n",
    "    # Computing the macro-average F1 for the second subtask\n",
    "    f1_macro = f1_score(ref, pred, average='macro', zero_division=0)\n",
    "    # Obtainig a detailed classification report\n",
    "    report = classification_report(ref,pred, digits=4, zero_division=0)\n",
    "    print(\"\\nSUBTASK 2\")\n",
    "    print(f\"F1-score (Macro-Averaged): {f1_macro}\")\n",
    "    print(report)\n",
    "\n",
    "\n",
    "###########################SUBTASK 1##################################\n",
    "\n",
    "# COMPLETE\n",
    "print(\"ENGLISH\\n\")\n",
    "print(\"STATIC EMBEDDINGS: \\n\")\n",
    "# Subtask 1\n",
    "print(\"SUBTASK 1\\n\")\n",
    "print(\"=================================\\n\")\n",
    "print(\"MLP: \")\n",
    "show_subtask1(y_test_task1, predicted1_1_static)\n",
    "\n",
    "print(\"=================================\\n\")\n",
    "print(\"XGBoost: \")\n",
    "show_subtask1(y_test_task1, predicted1_2_static)\n",
    "print(\"=================================\\n\")\n",
    "\n",
    "# Subtask 2\n",
    "print(\"SUBTASK 2\\n\")\n",
    "print(\"=================================\\n\")\n",
    "print(\"MLP: \")\n",
    "show_subtask2(y_test_task2, predicted2_1_static)\n",
    "print(\"=================================\\n\")\n",
    "print(\"XGBoost: \")\n",
    "show_subtask2(y_test_task2, predicted2_2_static)\n",
    "print(\"=================================\\n\")\n",
    "\n",
    "\n",
    "print(\"ENGLISH\\n\")\n",
    "print(\"CONTEXTUAL EMBEDDINGS: \\n\")\n",
    "# Subtask 1\n",
    "print(\"SUBTASK 1\\n\")\n",
    "print(\"=================================\\n\")\n",
    "print(\"MLP: \")\n",
    "show_subtask1(y_test_task1, predicted1_1_contextual)\n",
    "print(\"=================================\\n\")\n",
    "print(\"XGBoost: \")\n",
    "show_subtask1(y_test_task1, predicted1_2_contextual)\n",
    "print(\"=================================\\n\")\n",
    "\n",
    "# Subtask 2\n",
    "print(\"SUBTASK 2\\n\")\n",
    "print(\"=================================\\n\")\n",
    "print(\"MLP: \")\n",
    "show_subtask2(y_test_task2, predicted2_1_contextual)\n",
    "print(\"=================================\\n\")\n",
    "print(\"XGBoost: \")\n",
    "show_subtask2(y_test_task2, predicted2_2_contextual)\n",
    "print(\"=================================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPANISH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utiliza el mismo procedimiento descrito anteriormente dado que el único cambio realizado es el idioma empleado. Esto implica mantener los métodos de evaluación, métricas y modelos empleados previamente, aplicando exactamente los mismos pasos para asegurar la comparabilidad directa de los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignora capulloel problema youtuber denuncia acoso afecta gente izquierdas ejemplo video gamergate presenta normal acoso reciben fisher anita zöey amenazas bomba\n",
      "si comicsgate parece gamergate pues bien acoso si haciendo sabotaje personajes gustan entonces gracias darme razón lloricas ofendidos\n",
      "lee gamergate cambiado manera comunicamos internet fanboys halo tóxicos fanboys comunidades/juegos querido coger pauta\n",
      "entonces así mercado mejor hacer cambiarlo seguir alimentando machismo consumidores lugar apoyar gente víctimas gamergateacerca implica imperativo entonces entiendo buscaban\n",
      "aaah andrew dobson dedicó echar mierda gamergate puro estilo white knight deviantart echó encima\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "# FUNCIONES PARA PREPROCESAMIENTO DE TEXTO\n",
    "# Limpieza del texto en español\n",
    "def limpiar_texto_esp(texto):\n",
    "    texto = web_re.sub(\"\", texto)\n",
    "    texto = user_re.sub(\"\", texto)\n",
    "    texto = hashtag_re.sub(\"\", texto)\n",
    "    texto = re.sub(pattern_emoji, \"\", texto)\n",
    "    texto = re.sub(symbols, \"\", texto)\n",
    "    texto = re.sub(dates, \"\", texto)\n",
    "    texto = re.sub(decimal, \"\", texto)\n",
    "    texto = re.sub(hora, \"\", texto)\n",
    "    texto = re.sub(fechas_format, \"\", texto)\n",
    "    return texto.lower()\n",
    "\n",
    "# Tokenización de textos en español\n",
    "def tokenizar_esp(lista_textos):\n",
    "    lista_tokens = []\n",
    "    for texto in lista_textos:\n",
    "        texto_procesado = limpiar_texto_esp(texto)\n",
    "        tokens = nltk.tokenize.word_tokenize(texto_procesado, language=\"spanish\", preserve_line=False)\n",
    "        tokens_filtrados = [palabra for palabra in tokens if palabra not in stopw[\"spanish\"]]\n",
    "        lista_tokens.append(\" \".join(tokens_filtrados))\n",
    "    return lista_tokens\n",
    "\n",
    "# Tokenización de los conjuntos de entrenamiento y prueba\n",
    "tokenized_text[\"spanish_train_1\"] = tokenizar_esp(SpTrainTask1[1])\n",
    "tokenized_text[\"spanish_test_1\"] = tokenizar_esp(SpDevTask1[1])\n",
    "tokenized_text[\"spanish_train_2\"] = tokenizar_esp(SpTrainTask2[1])\n",
    "tokenized_text[\"spanish_test_2\"] = tokenizar_esp(SpDevTask2[1])\n",
    "\n",
    "tokenized_text[\"spanish_train_1\"] = tokenize(SpTrainTask1[1], \"spanish\")\n",
    "tokenized_text[\"spanish_test_1\"] = tokenize(SpDevTask1[1], \"spanish\")\n",
    "tokenized_text[\"spanish_train_2\"] = tokenize(SpTrainTask2[1], \"spanish\")\n",
    "tokenized_text[\"spanish_test_2\"] = tokenize(SpDevTask2[1], \"spanish\")\n",
    "\n",
    "# Mostrar ejemplos de los textos tokenizados\n",
    "for idx in range(min(5, len(tokenized_text[\"spanish_train_1\"]))):\n",
    "    print(tokenized_text[\"spanish_train_1\"][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones y vectorización de los datos\n",
    "datos_prueba_X_tarea1 = tokenized_text[\"spanish_test_1\"]\n",
    "X_vectores_prueba_tarea1 = vectorizer_task1.fit_transform(datos_prueba_X_tarea1)\n",
    "lsa_vectores_prueba_tarea1 = svd1.fit_transform(X_vectores_prueba_tarea1)\n",
    "\n",
    "datos_prueba_y_tarea1 = SpDevTask1[2].tolist()\n",
    "y_prueba_tarea1 = label_encoder_task1.fit_transform(datos_prueba_y_tarea1)\n",
    "\n",
    "datos_prueba_X_tarea2 = tokenized_text[\"spanish_test_2\"]\n",
    "X_vectores_prueba_tarea2 = vectorizer_task2.fit_transform(datos_prueba_X_tarea2)\n",
    "lsa_vectores_prueba_tarea2 = svd2.fit_transform(X_vectores_prueba_tarea2)\n",
    "\n",
    "datos_prueba_y_tarea2 = SpDevTask2[2].tolist()\n",
    "y_prueba_tarea2 = label_encoder_task2.fit_transform(datos_prueba_y_tarea2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet representations (Feature extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import numpy as np \n",
    "\n",
    "# Obtención de representaciones para los subconjuntos de entrenamiento y desarrollo en ambas tareas\n",
    "datos_y_tarea1 = SpTrainTask1[2].tolist()\n",
    "datos_X_tarea1, etiquetas_y_tarea1 = tokenized_text[\"spanish_train_1\"], datos_y_tarea1\n",
    "\n",
    "reductor_tarea1 = TruncatedSVD(n_components=100)\n",
    "vectorizador_tarea1 = TfidfVectorizer(norm=\"l2\")\n",
    "codificador_tarea1 = LabelEncoder()\n",
    "\n",
    "X_vectores_entrenamiento1 = vectorizador_tarea1.fit_transform(datos_X_tarea1)\n",
    "lsa_vectores_entrenamiento1 = reductor_tarea1.fit_transform(X_vectores_entrenamiento1)\n",
    "\n",
    "y_etiquetas_numericas1 = codificador_tarea1.fit_transform(etiquetas_y_tarea1)\n",
    "\n",
    "datos_y_tarea2 = SpTrainTask2[2].tolist()\n",
    "datos_X_tarea2, etiquetas_y_tarea2 = tokenized_text[\"spanish_train_2\"], datos_y_tarea2\n",
    "\n",
    "vectorizador_tarea2 = TfidfVectorizer(norm=\"l2\")\n",
    "codificador_tarea2 = LabelEncoder()\n",
    "reductor_tarea2 = TruncatedSVD(n_components=100)\n",
    "\n",
    "X_vectores_entrenamiento2 = vectorizador_tarea2.fit_transform(datos_X_tarea2)\n",
    "lsa_vectores_entrenamiento2 = reductor_tarea2.fit_transform(X_vectores_entrenamiento2)\n",
    "\n",
    "y_etiquetas_numericas2 = codificador_tarea2.fit_transform(etiquetas_y_tarea2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de lsa_vectores_entrenamiento1: (3194, 100)\n",
      "Forma de lsa_vectores_entrenamiento2: (1217, 100)\n"
     ]
    }
   ],
   "source": [
    "# Impresión de dimensiones de los vectores reducidos\n",
    "print(\"Forma de lsa_vectores_entrenamiento1:\", lsa_vectores_entrenamiento1.shape)\n",
    "print(\"Forma de lsa_vectores_entrenamiento2:\", lsa_vectores_entrenamiento2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de datos de prueba\n",
    "datos_prueba_X_tarea1 = tokenized_text[\"spanish_test_1\"]\n",
    "X_vectores_prueba_tarea1 = vectorizador_tarea1.transform(datos_prueba_X_tarea1)\n",
    "lsa_vectores_prueba_tarea1 = reductor_tarea1.transform(X_vectores_prueba_tarea1)\n",
    "\n",
    "datos_prueba_y_tarea1 = SpDevTask1[2].tolist()\n",
    "y_prueba_tarea1 = codificador_tarea1.fit_transform(datos_prueba_y_tarea1)\n",
    "\n",
    "datos_prueba_X_tarea2 = tokenized_text[\"spanish_test_2\"]\n",
    "X_vectores_prueba_tarea2 = vectorizador_tarea2.transform(datos_prueba_X_tarea2)\n",
    "lsa_vectores_prueba_tarea2 = reductor_tarea2.transform(X_vectores_prueba_tarea2)\n",
    "\n",
    "datos_prueba_y_tarea2 = SpDevTask2[2].tolist()\n",
    "y_prueba_tarea2 = codificador_tarea2.fit_transform(datos_prueba_y_tarea2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(490, 100)\n",
      "(207, 100)\n"
     ]
    }
   ],
   "source": [
    "print(lsa_vectores_prueba_tarea1.shape)\n",
    "print(lsa_vectores_prueba_tarea2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo: PlanTL-GOB-ES/roberta-base-bne, torch.Size([3194, 768])\n",
      "Modelo: PlanTL-GOB-ES/roberta-base-bne, torch.Size([1217, 768])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[212], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m entrada \u001b[38;5;241m=\u001b[39m entrada\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 32\u001b[0m     salidas \u001b[38;5;241m=\u001b[39m modelo(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mentrada)\n\u001b[0;32m     33\u001b[0m     capas_codificadas \u001b[38;5;241m=\u001b[39m salidas[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     34\u001b[0m     vector_cls \u001b[38;5;241m=\u001b[39m capas_codificadas[:,\u001b[38;5;241m0\u001b[39m,:]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:976\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    974\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 976\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    987\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    989\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:631\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    620\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    621\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    622\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    628\u001b[0m         output_attentions,\n\u001b[0;32m    629\u001b[0m     )\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 631\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    641\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:562\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    559\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    560\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 562\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    565\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    567\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\pytorch_utils.py:261\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:575\u001b[0m, in \u001b[0;36mRobertaLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[0;32m    574\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 575\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:486\u001b[0m, in \u001b[0;36mRobertaOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 486\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    488\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "tam_lote = 16  \n",
    "\n",
    "# COMPLETO\n",
    "tweets_totales = [tokenized_text[\"spanish_train_1\"], tokenized_text[\"spanish_train_2\"], tokenized_text[\"spanish_test_1\"], tokenized_text[\"spanish_test_2\"]]\n",
    "\n",
    "tarea = 1\n",
    "\n",
    "idioma = \"spanish\"\n",
    "\n",
    "for tweets in tweets_totales:\n",
    "    for i, modelo in enumerate(models[idioma]):\n",
    "        tokenizador = tokenizers[idioma][i]\n",
    "        modelo = models[idioma][i]\n",
    "        nombre = modelnames[idioma][i]\n",
    "\n",
    "        lista_tensores = []\n",
    "\n",
    "        for i in range(0, len(tweets), tam_lote):\n",
    "            lote = tweets[i:i+tam_lote]\n",
    "            entrada = tokenizador(lote, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            modelo.eval()\n",
    "            modelo.to(device)\n",
    "            entrada = entrada.to(device)\n",
    "            with torch.no_grad():\n",
    "                salidas = modelo(**entrada)\n",
    "                capas_codificadas = salidas[0]\n",
    "                vector_cls = capas_codificadas[:,0,:]\n",
    "            lista_tensores.append(vector_cls)\n",
    "\n",
    "        vector_cls = torch.cat(lista_tensores).cpu()\n",
    "\n",
    "        if tarea == 1:\n",
    "            vectors[\"task1\"][idioma][\"train\"][nombre] = vector_cls\n",
    "        elif tarea == 2:\n",
    "            vectors[\"task2\"][idioma][\"train\"][nombre] = vector_cls\n",
    "        elif tarea == 3:\n",
    "            vectors[\"task1\"][idioma][\"test\"][nombre] = vector_cls\n",
    "        else:\n",
    "            vectors[\"task2\"][idioma][\"test\"][nombre] = vector_cls\n",
    "\n",
    "        tarea += 1\n",
    "        print(f\"Modelo: {nombre}, {vector_cls.size()}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0164, -0.0334,  0.0596,  ..., -0.0298, -0.0328,  0.1693],\n",
       "        [-0.1075, -0.0646,  0.0651,  ..., -0.0653, -0.0259,  0.1355],\n",
       "        [-0.1203, -0.1275,  0.0947,  ..., -0.0390,  0.0355,  0.1365],\n",
       "        ...,\n",
       "        [-0.1062, -0.0527,  0.0321,  ..., -0.0448,  0.0261,  0.1514],\n",
       "        [-0.0356, -0.0579,  0.1164,  ..., -0.1082, -0.0406,  0.1134],\n",
       "        [-0.0650, -0.0079,  0.0311,  ..., -0.0564,  0.0343,  0.1082]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors[\"task2\"][\"spanish\"][\"train\"][\"PlanTL-GOB-ES/roberta-base-bne\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marcos\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\Users\\marcos\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# COMPLETO  \n",
    "### ESTÁTICO  \n",
    "\n",
    "clasificador_estatico_1 = SVC(random_state=1, max_iter=1000)  \n",
    "clasificador_estatico_1.fit(lsa_vectores_entrenamiento1, y_etiquetas_numericas1)  \n",
    "\n",
    "# Convertir datos a formato DMatrix  \n",
    "datos_entrenamiento_estatico_t1 = xgb.DMatrix(lsa_vectores_entrenamiento1, label=y_etiquetas_numericas1)  \n",
    "datos_prueba_estatico_t1 = xgb.DMatrix(lsa_vectores_prueba_tarea1, label=y_prueba_tarea1)  \n",
    "\n",
    "# Definir parámetros de XGBoost  \n",
    "parametros_estatico = {  \n",
    "    'max_depth': 3,  \n",
    "    'eta': 0.1,  \n",
    "    'objective': 'binary:logistic',  \n",
    "    'eval_metric': 'auc'  \n",
    "}  \n",
    "\n",
    "# Entrenar el modelo  \n",
    "rondas_estatico = 100  \n",
    "modelo_estatico_1 = xgb.train(parametros_estatico, datos_entrenamiento_estatico_t1, rondas_estatico)  \n",
    "\n",
    "\n",
    "### CONTEXTUAL  \n",
    "\n",
    "clasificador_contextual_1 = SVC(random_state=1, max_iter=1000)  \n",
    "clasificador_contextual_1.fit(vectors[\"task1\"][\"spanish\"][\"train\"][\"PlanTL-GOB-ES/roberta-base-bne\"], y_etiquetas_numericas1)  \n",
    "\n",
    "# Convertir datos a formato DMatrix  \n",
    "datos_entrenamiento_contextual_t1 = xgb.DMatrix(vectors[\"task1\"][\"spanish\"][\"train\"][\"PlanTL-GOB-ES/roberta-base-bne\"], label=y_etiquetas_numericas1)  \n",
    "datos_prueba_contextual_t1 = xgb.DMatrix(vectors[\"task1\"][\"spanish\"][\"test\"][\"PlanTL-GOB-ES/roberta-base-bne\"], label=y_prueba_tarea1)  \n",
    "\n",
    "# Definir parámetros de XGBoost  \n",
    "parametros_contextual = {  \n",
    "    'max_depth': 3,  \n",
    "    'eta': 0.1,  \n",
    "    'objective': 'binary:logistic',  \n",
    "    'eval_metric': 'auc'  \n",
    "}  \n",
    "\n",
    "# Entrenar el modelo  \n",
    "rondas_contextual = 100  \n",
    "modelo_contextual_1 = xgb.train(parametros_contextual, datos_entrenamiento_contextual_t1, rondas_contextual)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marcos\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Entrenamiento del clasificador SVM usando vectores estáticos (e.g., LSA)\n",
    "clasificador_estatico_2 = SVC(random_state=1, max_iter=1000)\n",
    "clasificador_estatico_2.fit(\n",
    "    lsa_vectores_entrenamiento2,  # Datos transformados con LSA para entrenamiento\n",
    "    y_etiquetas_numericas2        # Etiquetas numéricas correspondientes a las clases\n",
    ")\n",
    "\n",
    "# Preparación de datos para entrenamiento y prueba con XGBoost utilizando vectores estáticos\n",
    "# Convertimos los datos en formato DMatrix, formato eficiente para XGBoost\n",
    "datos_entrenamiento_estatico = xgb.DMatrix(\n",
    "    lsa_vectores_entrenamiento2,\n",
    "    label=y_etiquetas_numericas2\n",
    ")\n",
    "datos_prueba_estatico = xgb.DMatrix(\n",
    "    lsa_vectores_prueba_tarea2,\n",
    "    label=y_prueba_tarea2\n",
    ")\n",
    "\n",
    "# Parámetros de entrenamiento para XGBoost (vectores estáticos)\n",
    "params_estatico = {\n",
    "    'max_depth': 3,              # Profundidad máxima de los árboles\n",
    "    'eta': 0.1,                  # Tasa de aprendizaje\n",
    "    'objective': 'multi:softmax',# Objetivo para clasificación multiclase\n",
    "    'num_class': 3,              # Número de clases objetivo\n",
    "    'eval_metric': 'mlogloss'    # Métrica de evaluación\n",
    "}\n",
    "num_rounds_estatico = 100        # Número de iteraciones del entrenamiento\n",
    "\n",
    "# Entrenamiento del modelo XGBoost con datos estáticos\n",
    "modelo_estatico_2 = xgb.train(\n",
    "    params_estatico,\n",
    "    datos_entrenamiento_estatico,\n",
    "    num_rounds_estatico\n",
    ")\n",
    "\n",
    "# Entrenamiento del clasificador SVM usando embeddings contextuales (RoBERTa-BNE)\n",
    "clasificador_contextual_2 = SVC(random_state=1, max_iter=1000)\n",
    "clasificador_contextual_2.fit(\n",
    "    vectors[\"task2\"][\"spanish\"][\"train\"][\"PlanTL-GOB-ES/roberta-base-bne\"],  # Embeddings contextuales de entrenamiento\n",
    "    datos_y_tarea2                                                            # Etiquetas numéricas correspondientes\n",
    ")\n",
    "\n",
    "# Conversión de embeddings contextuales a formato DMatrix para XGBoost\n",
    "datos_entrenamiento_contextual = xgb.DMatrix(\n",
    "    vectors[\"task2\"][\"spanish\"][\"train\"][\"PlanTL-GOB-ES/roberta-base-bne\"],\n",
    "    label=y_etiquetas_numericas2\n",
    ")\n",
    "datos_prueba_contextual = xgb.DMatrix(\n",
    "    vectors[\"task2\"][\"spanish\"][\"test\"][\"PlanTL-GOB-ES/roberta-base-bne\"],\n",
    "    label=y_prueba_tarea2\n",
    ")\n",
    "\n",
    "# Parámetros de entrenamiento para XGBoost usando embeddings contextuales\n",
    "params_contextual = {\n",
    "    'max_depth': 3,              # Profundidad máxima de los árboles\n",
    "    'eta': 0.1,                  # Tasa de aprendizaje\n",
    "    'objective': 'multi:softmax',# Objetivo para clasificación multiclase\n",
    "    'num_class': 3,              # Número de clases objetivo\n",
    "    'eval_metric': 'mlogloss'    # Métrica de evaluación\n",
    "}\n",
    "num_rounds_contextual = 100      # Número de iteraciones del entrenamiento\n",
    "\n",
    "# Entrenamiento del modelo XGBoost con embeddings contextuales\n",
    "modelo_contextual_2 = xgb.train(\n",
    "    params_contextual,\n",
    "    datos_entrenamiento_contextual,\n",
    "    num_rounds_contextual\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicho1_1_estatico = clasificador_estatico_1.predict(lsa_vectores_prueba_tarea1)  \n",
    "\n",
    "predicho1_1_contextual = clasificador_contextual_1.predict(vectors[\"task1\"][\"spanish\"][\"test\"][\"PlanTL-GOB-ES/roberta-base-bne\"])  \n",
    "\n",
    "# Generar predicciones en el conjunto de prueba  \n",
    "predicho1_2_estatico = modelo_estatico_1.predict(datos_prueba_estatico)  \n",
    "predicho1_2_estatico = np.asarray([int(round(valor)) for valor in predicho1_2_estatico])  \n",
    "\n",
    "# Generar predicciones en el conjunto de prueba  \n",
    "predicho1_2_contextual = modelo_contextual_1.predict(datos_prueba_contextual)  \n",
    "predicho1_2_contextual = np.asarray([int(round(valor)) for valor in predicho1_2_contextual])  \n",
    "\n",
    "\n",
    "# Generar predicciones para EnDevTask2 usando los modelos _2  \n",
    "datos_prueba_tarea2 = xgb.DMatrix(lsa_vectores_prueba_tarea2, label=y_prueba_tarea2)  \n",
    "\n",
    "predicho2_1_estatico = clasificador_estatico_2.predict(lsa_vectores_prueba_tarea2)  \n",
    "\n",
    "predicho2_1_contextual = clasificador_contextual_2.predict(vectors[\"task2\"][\"spanish\"][\"test\"][\"PlanTL-GOB-ES/roberta-base-bne\"])  \n",
    "\n",
    "# Generar predicciones en el conjunto de prueba  \n",
    "predicho2_2_estatico = modelo_estatico_2.predict(datos_prueba_tarea2)  \n",
    "predicho2_2_estatico = np.asarray([int(round(valor)) for valor in predicho2_2_estatico])  \n",
    "\n",
    "# Generar predicciones en el conjunto de prueba  \n",
    "predicho2_2_contextual = modelo_contextual_2.predict(datos_prueba_contextual)  \n",
    "predicho2_2_contextual = np.asarray([int(round(valor)) for valor in predicho2_2_contextual])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción utilizando clasificador estático (SVM) para la tarea 1\n",
    "predicho1_1_estatico = clasificador_estatico_1.predict(lsa_vectores_prueba_tarea1)\n",
    "\n",
    "# Predicción utilizando clasificador contextual (SVM) con embeddings de RoBERTa para la tarea 1\n",
    "predicho1_1_contextual = clasificador_contextual_1.predict(\n",
    "    vectors[\"task1\"][\"spanish\"][\"test\"][\"PlanTL-GOB-ES/roberta-base-bne\"]\n",
    ")\n",
    "\n",
    "# Predicción utilizando modelo estático (XGBoost) para la tarea 1\n",
    "predicho1_2_estatico = modelo_estatico_1.predict(datos_prueba_estatico)\n",
    "# Conversión de predicciones a valores enteros\n",
    "predicho1_2_estatico = np.asarray([int(round(valor)) for valor in predicho1_2_estatico])\n",
    "\n",
    "# Predicción utilizando modelo contextual (XGBoost) con embeddings para la tarea 1\n",
    "predicho1_2_contextual = modelo_contextual_1.predict(datos_prueba_contextual)\n",
    "# Conversión de predicciones a valores enteros\n",
    "predicho1_2_contextual = np.asarray([int(round(valor)) for valor in predicho1_2_contextual])\n",
    "\n",
    "# Predicción utilizando clasificador estático (SVM) para la tarea 2\n",
    "predicho2_1_estatico = clasificador_estatico_2.predict(lsa_vectores_prueba_tarea2)\n",
    "\n",
    "# Predicción utilizando clasificador contextual (SVM) con embeddings de RoBERTa para la tarea 2\n",
    "predicho2_1_contextual = clasificador_contextual_2.predict(\n",
    "    vectors[\"task2\"][\"spanish\"][\"test\"][\"PlanTL-GOB-ES/roberta-base-bne\"]\n",
    ")\n",
    "\n",
    "# Predicción utilizando modelo estático (XGBoost) para la tarea 2\n",
    "predicho2_2_estatico = modelo_estatico_2.predict(datos_prueba_estatico)\n",
    "# Conversión de predicciones a valores enteros\n",
    "predicho2_2_estatico = np.asarray([int(round(valor)) for valor in predicho2_2_estatico])\n",
    "\n",
    "# Predicción utilizando modelo contextual (XGBoost) con embeddings para la tarea 2\n",
    "predicho2_2_contextual = modelo_contextual_2.predict(datos_prueba_contextual)\n",
    "# Conversión de predicciones a valores enteros\n",
    "predicho2_2_contextual = np.asarray([int(round(valor)) for valor in predicho2_2_contextual])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Función para mostrar resultados de la tarea 1 (clasificación binaria)\n",
    "def mostrar_tarea1(referencia, prediccion):\n",
    "    # Calcula F1 para la clase positiva\n",
    "    f1_positivo = f1_score(referencia, prediccion, pos_label=1, zero_division=0)\n",
    "    # Genera un reporte de clasificación detallado\n",
    "    reporte = classification_report(referencia, prediccion, digits=4, zero_division=0)\n",
    "    print(\"\\nTAREA 1 - Resultados\")\n",
    "    print(f\"Puntuación F1 (Clase Positiva): {f1_positivo:.4f}\")\n",
    "    print(reporte)\n",
    "\n",
    "# Función para mostrar resultados de la tarea 2 (clasificación multiclase)\n",
    "def mostrar_tarea2(referencia, prediccion):\n",
    "    # Calcula F1 promedio macro (útil para clasificación multiclase)\n",
    "    f1_macro = f1_score(referencia, prediccion, average='macro', zero_division=0)\n",
    "    # Genera un reporte de clasificación detallado\n",
    "    reporte = classification_report(referencia, prediccion, digits=4, zero_division=0)\n",
    "    print(\"\\nTAREA 2 - Resultados\")\n",
    "    print(f\"Puntuación F1 (Promedio Macro): {f1_macro:.4f}\")\n",
    "    print(reporte)\n",
    "\n",
    "# Imprime encabezado principal\n",
    "print(\"==== RESULTADOS PARA ESPAÑOL ====\\n\")\n",
    "\n",
    "# Resultados usando embebimientos estáticos\n",
    "print(\"EMBEBIMIENTOS ESTÁTICOS\\n\")\n",
    "\n",
    "print(\"TAREA 1\\n=================================\\n\")\n",
    "print(\"SVM: \")\n",
    "mostrar_tarea1(y_prueba_tarea1, predicho1_1_estatico)\n",
    "\n",
    "print(\"=================================\\nXGBoost: \")\n",
    "mostrar_tarea1(y_prueba_tarea1, predicho1_2_estatico)\n",
    "print(\"=================================\\n\")\n",
    "\n",
    "print(\"TAREA 2\\n=================================\\n\")\n",
    "print(\"SVM: \")\n",
    "mostrar_tarea2(y_prueba_tarea2, predicho2_1_estatico)\n",
    "\n",
    "print(\"=================================\\nXGBoost: \")\n",
    "mostrar_tarea2(y_prueba_tarea2, predicho2_2_estatico)\n",
    "print(\"=================================\\n\")\n",
    "\n",
    "# Resultados usando embebimientos contextuales\n",
    "print(\"EMBEBIMIENTOS CONTEXTUALES\\n\")\n",
    "\n",
    "print(\"TAREA 1\\n=================================\\n\")\n",
    "print(\"SVM: \")\n",
    "mostrar_tarea1(y_prueba_tarea1, predicho1_1_contextual)\n",
    "\n",
    "print(\"=================================\\nXGBoost: \")\n",
    "mostrar_tarea1(y_prueba_tarea1, predicho1_2_contextual)\n",
    "print(\"=================================\\n\")\n",
    "\n",
    "print(\"TAREA 2\\n=================================\\n\")\n",
    "print(\"SVM: \")\n",
    "mostrar_tarea2(y_prueba_tarea2, predicho2_1_contextual)\n",
    "\n",
    "print(\"=================================\\nXGBoost: \")\n",
    "mostrar_tarea2(y_prueba_tarea2, predicho2_2_contextual)\n",
    "print(\"=================================\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
