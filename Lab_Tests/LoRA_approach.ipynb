{"cells":[{"cell_type":"markdown","id":"0b45a0e6","metadata":{"id":"0b45a0e6"},"source":["# LoRA with Hugging Models Approach"]},{"cell_type":"markdown","source":["## Libraries"],"metadata":{"id":"ouF3xvWANEKA"},"id":"ouF3xvWANEKA"},{"cell_type":"code","execution_count":null,"id":"ce6ee761","metadata":{"id":"ce6ee761"},"outputs":[],"source":["COLAB =True # IF YOU USE GOOGLE COLAB -> COLAB = True\n","PIP = True # IF YOU NEED INSTALL LIBRARIES -> PIP = True\n","\n","if PIP:\n","    !pip install transformers --upgrade\n","    !pip install datasets accelerate\n","    !pip install evaluate\n","    !pip install -U PyEvALL\n","\n","!pip install torch\n","!pip install numpy\n","!pip install pandas\n","!pip install scikit-learn\n","!pip install -U optuna"]},{"cell_type":"code","source":["# Standard libraries\n","import os\n","import sys\n","import tempfile\n","import time\n","import ast\n","import json\n","import random\n","\n","# Data manipulation\n","import numpy as np\n","import pandas as pd\n","\n","# PyTorch\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Transformers\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    AutoModelForCausalLM,\n","    Trainer,\n","    TrainingArguments,\n","    EarlyStoppingCallback\n",")\n","\n","# PEFT (Parameter-Efficient Fine-Tuning)\n","from peft import LoraConfig, get_peft_model, TaskType\n","\n","# Evaluation\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, f1_score\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","\n","# Optuna for hyperparameter tuning\n","import optuna\n","\n","# PyEvALL for evaluation\n","from pyevall.evaluation import PyEvALLEvaluation\n","from pyevall.metrics.metricfactory import MetricFactory\n","from pyevall.reports.reports import PyEvALLReport\n","from pyevall.utils.utils import PyEvALLUtils\n"],"metadata":{"id":"s8Emyfl2NFBx"},"id":"s8Emyfl2NFBx","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Drive and Dataset"],"metadata":{"id":"GShSbwY0NPch"},"id":"GShSbwY0NPch"},{"cell_type":"code","source":["from pathlib import Path\n","\n","if COLAB is True:\n","  from google.colab import drive\n","  drive.mount('/content/drive',force_remount=True)\n","  base_path = \"/content/drive/MyDrive/EXISTS2025_TweetBusters\"\n","  library_path = base_path + \"/Functions\"\n","else:\n","  base_path = Path.cwd().parent\n","  library_path = base_path / \"Functions\"\n","\n","\n","\n","sys.path.insert(0, str(library_path))\n","from readerEXIST2025_2 import EXISTReader"],"metadata":{"id":"Vgz__f7TNQlG"},"id":"Vgz__f7TNQlG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# path to the dataset, adapt this path wherever you have the dataset\n","dataset_path = os.path.join(base_path, \"Dataset/EXIST_2025_Dataset_V0.3/\")\n","\n","file_train = os.path.join(dataset_path, \"EXIST2025_training.json\")\n","file_dev = os.path.join(dataset_path, \"EXIST2025_dev.json\")\n","file_test = os.path.join(dataset_path, \"EXIST2025_test_clean.json\")\n","\n","\n","reader_train = EXISTReader(file_train)\n","reader_dev = EXISTReader(file_dev)\n","reader_test = EXISTReader(file_test)\n","\n","\n","EnTrainTask1, EnDevTask1, EnTestTask1 = reader_train.get(lang=\"EN\", subtask=\"1\"), reader_dev.get(lang=\"EN\", subtask=\"1\"), reader_test.get(lang=\"EN\", subtask=\"1\")\n","EnTrainTask2, EnDevTask2, EnTestTask2 = reader_train.get(lang=\"EN\", subtask=\"2\"), reader_dev.get(lang=\"EN\", subtask=\"2\"), reader_test.get(lang=\"EN\", subtask=\"2\")\n","EnTrainTask3, EnDevTask3, EnTestTask3 = reader_train.get(lang=\"EN\", subtask=\"3\"), reader_dev.get(lang=\"EN\", subtask=\"3\"), reader_test.get(lang=\"EN\", subtask=\"3\")\n","\n","\n","SpTrainTask1, SpDevTask1, SpTestTask1  = reader_train.get(lang=\"ES\", subtask=\"1\"), reader_dev.get(lang=\"ES\", subtask=\"1\"), reader_test.get(lang=\"ES\", subtask=\"1\")\n","SpTrainTask2, SpDevTask2, SpTestTask2  = reader_train.get(lang=\"ES\", subtask=\"2\"), reader_dev.get(lang=\"ES\", subtask=\"2\"), reader_test.get(lang=\"ES\", subtask=\"2\")\n","SpTrainTask3, SpDevTask3, SpTestTask3  = reader_train.get(lang=\"ES\", subtask=\"3\"), reader_dev.get(lang=\"ES\", subtask=\"3\"), reader_test.get(lang=\"ES\", subtask=\"3\")\n"],"metadata":{"id":"39sdXycuNR9z"},"id":"39sdXycuNR9z","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import Code Functions"],"metadata":{"id":"EvnqCdoLNT8y"},"id":"EvnqCdoLNT8y"},{"cell_type":"code","source":["import os\n","import importlib.util\n","import sys\n","import inspect\n","\n","functions_path = library_path\n","\n","for filename in os.listdir(functions_path):\n","    if filename.endswith(\".py\") and not filename.startswith(\"__\"):\n","        module_name = filename[:-3]\n","        file_path = os.path.join(functions_path, filename)\n","\n","        # Cargar el módulo\n","        spec = importlib.util.spec_from_file_location(module_name, file_path)\n","        module = importlib.util.module_from_spec(spec)\n","        spec.loader.exec_module(module)\n","\n","        # Extraer todas las funciones del módulo y cargarlas al espacio global\n","        for name, func in inspect.getmembers(module, inspect.isfunction):\n","            globals()[name] = func  # o locals()[name] si estás dentro de una función"],"metadata":{"id":"6JpEaFqyNVqU"},"id":"6JpEaFqyNVqU","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Seeding"],"metadata":{"id":"sxmKTjTCNeg2"},"id":"sxmKTjTCNeg2"},{"cell_type":"code","source":["def set_seed(seed=1234):\n","    \"\"\"\n","    Sets the seed to make everything deterministic, for reproducibility of experiments\n","    Parameters:\n","    seed: the number to set the seed to\n","    Return: None\n","    \"\"\"\n","    # Random seed\n","    random.seed(seed)\n","    # Numpy seed\n","    np.random.seed(seed)\n","    # Torch seed\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","    # os seed\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n"],"metadata":{"id":"GeUB0_GGNiFP"},"id":"GeUB0_GGNiFP","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exploratory Model Selection"],"metadata":{"id":"FdJXMmIp7M1n"},"id":"FdJXMmIp7M1n"},{"cell_type":"markdown","source":["### Task 1"],"metadata":{"id":"VKnrGfCM7cFN"},"id":"VKnrGfCM7cFN"},{"cell_type":"code","source":["import numpy as np\n","import optuna\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer\n",")\n","from peft import LoraConfig, get_peft_model, TaskType\n","from sklearn.preprocessing import LabelEncoder\n","\n","model_names = [\n","    \"cardiffnlp/twitter-roberta-base-2022-154m\",\n","    \"cardiffnlp/twitter-roberta-large-2022-154m\",\n","    \"cardiffnlp/twitter-xlm-roberta-base\",\n","    \"cardiffnlp/twitter-roberta-base\",\n","    \"sdadas/xlm-roberta-large-twitter\",\n","    \"g8a9/distilroberta-base-twitter-16M_aug-oct22\",\n","    \"andrea-t94/roberta-fine-tuned-twitter\",\n","    \"bdotloh/twitter-roberta-base-finetuned-twitter-user-desc\"\n","]\n","\n","params_twitter_roberta = {\n","    \"num_train_epochs\": 100,\n","    \"learning_rate\": 0.001,\n","    \"per_device_train_batch_size\": 8,\n","    \"per_device_eval_batch_size\": 8,\n","    \"logging_steps\": 100,\n","}\n","\n","# Suponiendo que ya tienes EnTrainTask2, EnDevTask2 definidos:\n","df_metrics = run_lora_experiments(\n","    task_num=1,\n","    model_names=model_names,\n","    params=params_twitter_roberta,\n","    trainInfo=EnTrainTask1,\n","    devInfo=EnDevTask1,\n","    testInfo=None\n",")\n","\n","print(df_metrics)\n","best_model_name = select_best_model(df_metrics, 1)\n","\n"],"metadata":{"id":"KCrBn3Yp7IXN"},"id":"KCrBn3Yp7IXN","execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_model, final_metrics = sexism_classification_pipeline_task1_LoRA(\n","    EnTrainTask1, EnDevTask1, None,\n","    best_model_name, 2, \"single_label_classification\",\n","    **optimized\n",")"],"metadata":{"id":"Xy-BIgwH7iVx"},"id":"Xy-BIgwH7iVx","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Task 2"],"metadata":{"id":"i8edgoBk7mLe"},"id":"i8edgoBk7mLe"},{"cell_type":"code","source":["import numpy as np\n","import optuna\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer\n",")\n","from peft import LoraConfig, get_peft_model, TaskType\n","from sklearn.preprocessing import LabelEncoder\n","\n","model_names = [\n","    \"cardiffnlp/twitter-roberta-base-2022-154m\",\n","    \"cardiffnlp/twitter-roberta-large-2022-154m\",\n","    \"cardiffnlp/twitter-xlm-roberta-base\",\n","    \"cardiffnlp/twitter-roberta-base\",\n","    \"sdadas/xlm-roberta-large-twitter\",\n","    \"g8a9/distilroberta-base-twitter-16M_aug-oct22\",\n","    \"andrea-t94/roberta-fine-tuned-twitter\",\n","    \"bdotloh/twitter-roberta-base-finetuned-twitter-user-desc\"\n","]\n","\n","params_twitter_roberta = {\n","    \"num_train_epochs\": 100,\n","    \"learning_rate\": 0.001,\n","    \"per_device_train_batch_size\": 8,\n","    \"per_device_eval_batch_size\": 8,\n","    \"logging_steps\": 100,\n","}\n","\n","# Suponiendo que ya tienes EnTrainTask2, EnDevTask2 definidos:\n","df_metrics = run_lora_experiments(\n","    task_num=2,\n","    model_names=model_names,\n","    params=params_twitter_roberta,\n","    trainInfo=EnTrainTask2,\n","    devInfo=EnDevTask2,\n","    testInfo=None\n",")\n","\n","\n","print(df_metrics)\n","best_model_name = select_best_model(df_metrics, 2)\n","print(f\"→ Modelo ganador: {best_model_name}\")\n","\n"],"metadata":{"id":"l8eXD9Nj7oMG"},"id":"l8eXD9Nj7oMG","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import optuna\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    TrainingArguments,\n","    Trainer\n",")\n","from peft import LoraConfig, get_peft_model, TaskType\n","from sklearn.preprocessing import LabelEncoder\n","\n","params_twitter_roberta = {\n","    \"num_train_epochs\": 100,\n","    \"learning_rate\": 0.001,\n","    \"per_device_train_batch_size\": 8,\n","    \"per_device_eval_batch_size\": 8,\n","    \"logging_steps\": 100,\n","}\n","\n","best_model_name = \"cardiffnlp/twitter-roberta-base\"\n","\n","optimized_2 = optimize_lora_hyperparams(\n","    task_num=2,\n","    best_model_name=best_model_name,\n","    params_base=params_twitter_roberta,\n","    trainInfo=EnTrainTask2,\n","    devInfo=EnDevTask2,\n","    n_trials=20\n",")\n","\n","\n","print(optimized_2)\n","# [I 2025-05-13 18:08:16,826] Trial 4 finished with value: 0.5675971449024997 and parameters: {'learning_rate': 0.0003822078857884255, 'r': 11, 'lora_alpha': 29}. Best is trial 4 with value: 0.5675971449024997.\n","#[I 2025-05-13 20:24:40,021] Trial 4 finished with value: 0.5597586424127885 and parameters: {'learning_rate': 9.1657096479339e-05, 'r': 8, 'lora_alpha': 59}. Best is trial 4 with value: 0.5597586424127885.\n","\n","\n","# prompt: #[I 2025-05-13 20:24:40,021] Trial 4 finished with value: 0.5597586424127885 and parameters: {'learning_rate': 9.1657096479339e-05, 'r': 8, 'lora_alpha': 59}. Best is trial 4 with value: 0.5597586424127885.\n","# convierte junto con params twitter  aun diccionario llamado optimized_2, tienes que inlcuir los parms twitter tambie\n","\n","optimized_2 = {\n","    'num_train_epochs': 100,\n","    'learning_rate': 9.1657096479339e-05,\n","    'per_device_train_batch_size': 8,\n","    'per_device_eval_batch_size': 8,\n","    'logging_steps': 100,\n","    'r': 8,\n","    'lora_alpha': 59\n","}\n"],"metadata":{"id":"f4bQ6mj37uLg"},"id":"f4bQ6mj37uLg","execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_model_2, final_metrics_2 = sexism_classification_pipeline_task2_LoRA(\n","    EnTrainTask2, EnDevTask2, None,\n","    best_model_name, 4, \"single_label_classification\",\n","    **optimized_2\n",")\n"],"metadata":{"id":"lopRfOkk71Az"},"id":"lopRfOkk71Az","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# prompt: haz un pipeline para la seleccion de un mejor modelo para la task3 probando difernetes repositorios de transformers de hugging face\n","\n","from functools import partial\n","from sklearn.preprocessing import MultiLabelBinarizer\n","\n","class SexismDatasetMulti(Dataset):\n","    def __init__(self, texts, labels, ids, tokenizer, max_len=128, pad=\"max_length\", trunc=True,rt='pt'):\n","        self.texts = texts.tolist()\n","        self.labels = labels\n","        self.ids = ids\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        self.pad = pad\n","        self.trunc = trunc\n","        self.rt = rt\n","\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_len,padding=self.pad, truncation=self.trunc,\n","            return_tensors=self.rt\n","        )\n","\n","        return {\n","            'input_ids': inputs['input_ids'].flatten(),\n","            'attention_mask': inputs['attention_mask'].flatten(),\n","            'labels': torch.tensor(self.labels[idx], dtype=torch.float),\n","            'id': torch.tensor(self.ids[idx], dtype=torch.long)}\n","# Ejemplo de uso para task 3:\n","\n","model_names = [\n","    \"cardiffnlp/twitter-roberta-base-2022-154m\",\n","    \"cardiffnlp/twitter-roberta-large-2022-154m\",\n","    \"cardiffnlp/twitter-xlm-roberta-base\",\n","    \"cardiffnlp/twitter-roberta-base\",\n","    \"sdadas/xlm-roberta-large-twitter\",\n","    \"g8a9/distilroberta-base-twitter-16M_aug-oct22\",\n","    \"andrea-t94/roberta-fine-tuned-twitter\",\n","    \"bdotloh/twitter-roberta-base-finetuned-twitter-user-desc\"\n","]\n","\n","params_task3 = {\n","    \"num_train_epochs\": 10,  # Ajusta según sea necesario\n","    \"learning_rate\": 5e-5,   # Ajusta según sea necesario\n","    \"per_device_train_batch_size\": 8,\n","    \"per_device_eval_batch_size\": 16,\n","    \"logging_steps\": 10,\n","    \"early_stopping_patience\": 3\n","}\n","\n","\n","df_metrics_task3 = run_lora_experiments(\n","    task_num=3,\n","    model_names=model_names,\n","    params=params_task3,\n","    trainInfo=EnTrainTask3,\n","    devInfo=EnDevTask3,\n","    testInfo=None  # Configura testInfo si tienes datos de test\n",")\n","\n","print(df_metrics_task3)\n","\n","best_model_task3 = select_best_model(df_metrics_task3, 3)\n","print(f\"→ Best model for task 3: {best_model_task3}\")\n"],"metadata":{"id":"BcGRr6It760f"},"id":"BcGRr6It760f","execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","optimized_task3 = optimize_lora_hyperparams(\n","    task_num=3,\n","    best_model_name=best_model_task3,\n","    params_base=params_task3,\n","    trainInfo=EnTrainTask3,\n","    devInfo=EnDevTask3,\n","    n_trials=5 # Reduce el número de trials para pruebas\n",")\n","\n","print(optimized_task3)\n","\n","final_model_task3, final_metrics_task3 = sexism_classification_pipeline_task3_LoRA(\n","    EnTrainTask3, EnDevTask3, None,\n","    best_model_task3, 5, \"multi_label_classification\",\n","    **optimized_task3\n",")"],"metadata":{"id":"wmJJS8L778jr"},"id":"wmJJS8L778jr","execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}